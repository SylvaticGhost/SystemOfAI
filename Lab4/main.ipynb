{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:46.747720Z",
     "start_time": "2025-11-12T22:35:45.597826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "import numba"
   ],
   "id": "607ff9f21f6f4806",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:46.757104Z",
     "start_time": "2025-11-12T22:35:46.752730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONST_COLOR_PLAYER = (240, 170, 103)\n",
    "CONST_COLOR_WALL = (84, 92, 214)\n",
    "CONST_COLOR_ENEMY = (210, 210, 64)\n",
    "\n",
    "CAT_EMPTY = 0\n",
    "CAT_PLAYER = 1\n",
    "CAT_WALL = 2\n",
    "CAT_ENEMY = 3"
   ],
   "id": "6e260a753c58a77",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:46.771921Z",
     "start_time": "2025-11-12T22:35:46.762619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@numba.njit\n",
    "def prepare_state_categorical_inner(obs_resized, h, w):\n",
    "    new_obs = np.full((h, w, 4), 0, dtype=np.uint8)\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            pixel = obs_resized[i, j]\n",
    "\n",
    "            # Compare pixel (which is a 1D np.array) to the tuple elements\n",
    "            if (pixel[0] == CONST_COLOR_PLAYER[0] and\n",
    "                pixel[1] == CONST_COLOR_PLAYER[1] and\n",
    "                pixel[2] == CONST_COLOR_PLAYER[2]):\n",
    "                new_obs[i, j, CAT_PLAYER] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_WALL[0] and\n",
    "                  pixel[1] == CONST_COLOR_WALL[1] and\n",
    "                  pixel[2] == CONST_COLOR_WALL[2]):\n",
    "                new_obs[i, j, CAT_WALL] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_ENEMY[0] and\n",
    "                  pixel[1] == CONST_COLOR_ENEMY[1] and\n",
    "                  pixel[2] == CONST_COLOR_ENEMY[2]):\n",
    "                new_obs[i, j, CAT_ENEMY] = 1\n",
    "            else:\n",
    "                new_obs[i, j, CAT_EMPTY] = 1\n",
    "    return new_obs\n",
    "\n",
    "def prepare_state_categorical(frame, h=21, w=21):\n",
    "    obs_resized = cv2.resize(frame, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return prepare_state_categorical_inner(obs_resized, h, w)"
   ],
   "id": "db176227558f2a08",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:46.780763Z",
     "start_time": "2025-11-12T22:35:46.776443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env, h=21, w=21):\n",
    "        super().__init__(env)\n",
    "        self.h, self.w = h, w\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (h, w, 4), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return prepare_state_categorical(obs, self.h, self.w)\n"
   ],
   "id": "c0da2a556648f73a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.775093Z",
     "start_time": "2025-11-12T22:35:46.785034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "env = ResizeObservation(env, h=21, w=21)\n",
    "observation, info = env.reset()\n"
   ],
   "id": "88ee4fa8a356cbef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.876533Z",
     "start_time": "2025-11-12T22:35:47.873010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"action_space:\", env.action_space)\n",
    "print(\"n actions:\", env.action_space.n)"
   ],
   "id": "c549efd9095a5b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(18)\n",
      "n actions: 18\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.901991Z",
     "start_time": "2025-11-12T22:35:47.896597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    meanings = env.unwrapped.get_action_meanings()\n",
    "except Exception:\n",
    "    try:\n",
    "        meanings = env.get_action_meanings()\n",
    "    except Exception:\n",
    "        meanings = None\n",
    "\n",
    "if meanings:\n",
    "    print(\"Action index -> meaning:\")\n",
    "    for i, name in enumerate(meanings):\n",
    "        print(f\"{i}: {name}\")\n",
    "else:\n",
    "    print(\"No action meanings available from the env. Use index numbers (0..n-1).\")\n"
   ],
   "id": "5f62493b12cd715e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action index -> meaning:\n",
      "0: NOOP\n",
      "1: FIRE\n",
      "2: UP\n",
      "3: RIGHT\n",
      "4: LEFT\n",
      "5: DOWN\n",
      "6: UPRIGHT\n",
      "7: UPLEFT\n",
      "8: DOWNRIGHT\n",
      "9: DOWNLEFT\n",
      "10: UPFIRE\n",
      "11: RIGHTFIRE\n",
      "12: LEFTFIRE\n",
      "13: DOWNFIRE\n",
      "14: UPRIGHTFIRE\n",
      "15: UPLEFTFIRE\n",
      "16: DOWNRIGHTFIRE\n",
      "17: DOWNLEFTFIRE\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.924164Z",
     "start_time": "2025-11-12T22:35:47.919125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIRE_ACTIONS = [1, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "MOVE_ACTIONS = [2, 3, 4, 5, 6, 7, 8, 9]"
   ],
   "id": "893650ccbe022de3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.939227Z",
     "start_time": "2025-11-12T22:35:47.934787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "id": "c49df2e1ad97e294",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.963443Z",
     "start_time": "2025-11-12T22:35:47.944758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-5\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    feature_h, feature_w = 21, 21\n",
    "    use_traces = False\n",
    "    lmbda = 0.9\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.state_dim = self.feature_h * self.feature_w * 4\n",
    "        feature_dim = self.state_dim + n_actions\n",
    "        self.w = np.zeros(feature_dim, dtype=np.float32)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        a_onehot = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        a_onehot[action] = 1.0\n",
    "        return np.concatenate([features, a_onehot])\n",
    "\n",
    "    def q_value(self, phi):\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        q_base = np.dot(state_features, self.w[:self.state_dim])\n",
    "        return q_base + self.w[self.state_dim:]\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        eps = float(getattr(self, \"epsilon\", 0.0))\n",
    "        eps = max(0.0, min(1.0, eps))\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "        return action, q_vals\n",
    "\n",
    "    def save(self, file_name=\"sarsa_weights.npz\"):\n",
    "        np.savez(file_name, w=self.w)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_name=\"sarsa_weights.npz\"):\n",
    "        data = np.load(file_name)\n",
    "        n_featues = 21*21*4\n",
    "        ag = Sarsa(n_actions=data['w'].shape[0] - n_featues)\n",
    "        ag.w = data['w']\n",
    "\n",
    "        print(\"Loaded SARSA agent with rules characteristics:\")\n",
    "        print(\"w shape:\", ag.w.shape)\n",
    "        print(\"w norm:\", np.linalg.norm(ag.w))\n",
    "        print(\"non-zero weights:\", np.count_nonzero(ag.w))\n",
    "        return ag\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0"
   ],
   "id": "23f7d9eb90e736ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.972960Z",
     "start_time": "2025-11-12T22:35:47.969449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_model_trained():\n",
    "    try:\n",
    "        _ = np.load(\"sarsa_weights.npz\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "3ea75ff7bcd035e6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.981638Z",
     "start_time": "2025-11-12T22:35:47.977641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file_exist(file_name):\n",
    "    try:\n",
    "        _ = np.load(file_name)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "342b5ae0c6a0411e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:47.995292Z",
     "start_time": "2025-11-12T22:35:47.987658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_LIVING = -0.1            # MANUAL: Must be a small negative \"living penalty\".\n",
    "R_INACTION = -0.4603       # IRL: Learned from your data.\n",
    "R_WALL = -0.6082           # IRL: Learned from your data.\n",
    "R_PROXIMITY = -0.0499      # IRL: Learned from your data.\n",
    "R_HUNTING = 0.2            # MANUAL: Override. The model failed to learn this.\n",
    "R_KILL = 15.0              # MANUAL: Override. The model failed to learn this.\n",
    "R_DEATH = -10.0            # MANUAL: Override. The model's value was too small.\n",
    "\n",
    "class StateObserver:\n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.prev_pos = None\n",
    "        self.prev_num_enemies = 0\n",
    "        self.prev_min_distance = None\n",
    "\n",
    "    def _find_entities(self, state):\n",
    "        player_coords = np.argwhere(state[:, :, CAT_PLAYER] == 1)\n",
    "        enemy_coords = np.argwhere(state[:, :, CAT_ENEMY] == 1)\n",
    "        player_pos = tuple(player_coords[0]) if len(player_coords) > 0 else None\n",
    "        return player_pos, enemy_coords\n",
    "\n",
    "    def analyze_state(self, state, reward, action):\n",
    "        player_pos, enemies = self._find_entities(state)\n",
    "        num_enemies = len(enemies)\n",
    "\n",
    "        # Start with the real reward from the game\n",
    "        shaped_reward = 0\n",
    "\n",
    "        # 1. Living Penalty: Small cost for every step to encourage speed.\n",
    "        shaped_reward += R_LIVING\n",
    "\n",
    "        if action not in MOVE_ACTIONS:\n",
    "            shaped_reward += R_INACTION\n",
    "\n",
    "        if (action in MOVE_ACTIONS and\n",
    "            player_pos is not None and\n",
    "            player_pos == self.prev_pos):\n",
    "            shaped_reward -= R_WALL\n",
    "\n",
    "        if num_enemies < self.prev_num_enemies:\n",
    "            shaped_reward += R_KILL\n",
    "\n",
    "        # 4. Proximity Penalty: Penalize being too close to an enemy.\n",
    "        if player_pos is not None and num_enemies > 0:\n",
    "            distances = np.sum(np.abs(enemies - player_pos), axis=1)\n",
    "            min_distance = np.min(distances)\n",
    "\n",
    "            if min_distance <= 2: # Too close!\n",
    "                shaped_reward += R_PROXIMITY\n",
    "\n",
    "            if self.prev_min_distance is not None and min_distance < self.prev_min_distance:\n",
    "                shaped_reward += R_HUNTING\n",
    "\n",
    "            self.prev_min_distance = min_distance\n",
    "        else:\n",
    "            self.prev_min_distance = None\n",
    "\n",
    "        # 5. Death Penalty: Big penalty if player disappears.\n",
    "        if player_pos is None and self.prev_pos is not None:\n",
    "             # Player was alive last step, but is gone now\n",
    "             shaped_reward += R_DEATH\n",
    "\n",
    "        # Update memory for the next step\n",
    "        self.prev_pos = player_pos\n",
    "        self.prev_num_enemies = num_enemies\n",
    "        return shaped_reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.prev_pos = None\n",
    "        self.prev_num_enemies = 0\n",
    "        self.prev_min_distance = None\n"
   ],
   "id": "19f739f20f96bb61",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:35:48.065910Z",
     "start_time": "2025-11-12T22:35:48.000302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epsilon_min = 0.05, epsilon_decay_fraction = 0.999, initial_epsilon = 1.0):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_fraction = epsilon_decay_fraction\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_name_for_class(class_name):\n",
    "        return f\"sarsa-weights-{class_name.lower()}.npz\"\n",
    "\n",
    "    def train_if_needed(self, model, env, class_name, n_episodes=1000):\n",
    "        file_name = Trainer._file_name_for_class(class_name)\n",
    "        print(f'Checking for existing model file: {file_name}')\n",
    "        if not file_exist(file_name):\n",
    "            self.train(model, env, class_name, n_episodes)\n",
    "            return model\n",
    "\n",
    "        return Sarsa.load(file_name)\n",
    "\n",
    "    def train(self, model, env, class_name, n_episodes=1000):\n",
    "        print(f\"Training {class_name} agent...\")\n",
    "        state_observer = StateObserver(w=21, h=21)\n",
    "        action_counts = np.zeros(env.action_space.n, dtype=np.float32)\n",
    "\n",
    "        max_score_ever = -np.inf\n",
    "\n",
    "        model.epsilon = self.initial_epsilon\n",
    "        decay_episodes = int(n_episodes * self.epsilon_decay_fraction)\n",
    "        if decay_episodes > 0:\n",
    "             epsilon_decay_step = (self.initial_epsilon - self.epsilon_min) / decay_episodes\n",
    "        else:\n",
    "             epsilon_decay_step = 0\n",
    "        print(f\"Epsilon will decay from {self.initial_epsilon} to {self.epsilon_min} over {decay_episodes} episodes.\")\n",
    "\n",
    "        log_step = max(1, n_episodes // 100)\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state_observer.reset()\n",
    "            # Ensure state is properly flattened\n",
    "            features = np.array(state, dtype=np.float32).flatten()\n",
    "            action, q_values = model.epsilon_greedy(features)\n",
    "            action_counts[action] += 1\n",
    "            phi = model.phi_from_state_action(features, action)\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Ensure next_state is properly flattened\n",
    "                next_features = np.array(next_state, dtype=np.float32).flatten()\n",
    "                next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                action_counts[next_action] += 1\n",
    "                next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "                if len(q_values) != model.n_actions:\n",
    "                    raise ValueError(f\"Expected q_values of length {model.n_actions}, got {len(q_values)}\")\n",
    "\n",
    "                q = q_values[action]\n",
    "                q_next = next_q_values[next_action]\n",
    "\n",
    "                # Use the original state (2D array) for state observer\n",
    "                shaped_reward = state_observer.analyze_state(state, reward, action)\n",
    "                delta = shaped_reward + model.gamma * q_next - q\n",
    "                model.w += model.alpha * delta * phi\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                q_values = next_q_values\n",
    "                phi = next_phi\n",
    "                ep_reward += reward\n",
    "\n",
    "            new_epsilon = model.epsilon - epsilon_decay_step\n",
    "            model.epsilon = max(self.epsilon_min, new_epsilon)\n",
    "\n",
    "            if ep_reward > max_score_ever:\n",
    "                max_score_ever = ep_reward\n",
    "\n",
    "\n",
    "            if (episode + 1) % log_step == 0:\n",
    "                print(f\"Episode {episode+1}/{n_episodes}: Reward={ep_reward:.2f}, Eps={model.epsilon:.4f}\")\n",
    "\n",
    "        print(f'Action distribution during training: {action_counts}')\n",
    "        print(f\"Training completed. Max score ever: {max_score_ever:.2f}\")\n",
    "        model.save(self._file_name_for_class(class_name))\n"
   ],
   "id": "f185cf8af1717cf",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:37:51.243801Z",
     "start_time": "2025-11-12T22:35:48.072924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_NAME = \"Berzerk-Default\"\n",
    "\n",
    "agent = Sarsa(env.action_space.n)\n",
    "\n",
    "epsilon_min = 0.1\n",
    "\n",
    "trainer = Trainer(epsilon_min)\n",
    "agent = trainer.train_if_needed(agent, env, class_name=CLASS_NAME, n_episodes=1000)\n",
    "\n",
    "env.close()\n"
   ],
   "id": "e0f035ed87e04e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model file: sarsa-weights-berzerk-default.npz\n",
      "Training Berzerk-Default agent...\n",
      "Epsilon will decay from 1.0 to 0.1 over 999 episodes.\n",
      "Episode 10/1000: Reward=250.00, Eps=0.9910\n",
      "Episode 20/1000: Reward=300.00, Eps=0.9820\n",
      "Episode 30/1000: Reward=200.00, Eps=0.9730\n",
      "Episode 40/1000: Reward=100.00, Eps=0.9640\n",
      "Episode 50/1000: Reward=400.00, Eps=0.9550\n",
      "Episode 60/1000: Reward=0.00, Eps=0.9459\n",
      "Episode 70/1000: Reward=100.00, Eps=0.9369\n",
      "Episode 80/1000: Reward=50.00, Eps=0.9279\n",
      "Episode 90/1000: Reward=250.00, Eps=0.9189\n",
      "Episode 100/1000: Reward=410.00, Eps=0.9099\n",
      "Episode 110/1000: Reward=200.00, Eps=0.9009\n",
      "Episode 120/1000: Reward=250.00, Eps=0.8919\n",
      "Episode 130/1000: Reward=350.00, Eps=0.8829\n",
      "Episode 140/1000: Reward=200.00, Eps=0.8739\n",
      "Episode 150/1000: Reward=50.00, Eps=0.8649\n",
      "Episode 160/1000: Reward=250.00, Eps=0.8559\n",
      "Episode 170/1000: Reward=50.00, Eps=0.8468\n",
      "Episode 180/1000: Reward=250.00, Eps=0.8378\n",
      "Episode 190/1000: Reward=150.00, Eps=0.8288\n",
      "Episode 200/1000: Reward=250.00, Eps=0.8198\n",
      "Episode 210/1000: Reward=100.00, Eps=0.8108\n",
      "Episode 220/1000: Reward=230.00, Eps=0.8018\n",
      "Episode 230/1000: Reward=50.00, Eps=0.7928\n",
      "Episode 240/1000: Reward=300.00, Eps=0.7838\n",
      "Episode 250/1000: Reward=100.00, Eps=0.7748\n",
      "Episode 260/1000: Reward=150.00, Eps=0.7658\n",
      "Episode 270/1000: Reward=150.00, Eps=0.7568\n",
      "Episode 280/1000: Reward=300.00, Eps=0.7477\n",
      "Episode 290/1000: Reward=250.00, Eps=0.7387\n",
      "Episode 300/1000: Reward=150.00, Eps=0.7297\n",
      "Episode 310/1000: Reward=200.00, Eps=0.7207\n",
      "Episode 320/1000: Reward=50.00, Eps=0.7117\n",
      "Episode 330/1000: Reward=200.00, Eps=0.7027\n",
      "Episode 340/1000: Reward=100.00, Eps=0.6937\n",
      "Episode 350/1000: Reward=150.00, Eps=0.6847\n",
      "Episode 360/1000: Reward=100.00, Eps=0.6757\n",
      "Episode 370/1000: Reward=50.00, Eps=0.6667\n",
      "Episode 380/1000: Reward=0.00, Eps=0.6577\n",
      "Episode 390/1000: Reward=50.00, Eps=0.6486\n",
      "Episode 400/1000: Reward=150.00, Eps=0.6396\n",
      "Episode 410/1000: Reward=250.00, Eps=0.6306\n",
      "Episode 420/1000: Reward=250.00, Eps=0.6216\n",
      "Episode 430/1000: Reward=200.00, Eps=0.6126\n",
      "Episode 440/1000: Reward=100.00, Eps=0.6036\n",
      "Episode 450/1000: Reward=100.00, Eps=0.5946\n",
      "Episode 460/1000: Reward=50.00, Eps=0.5856\n",
      "Episode 470/1000: Reward=200.00, Eps=0.5766\n",
      "Episode 480/1000: Reward=100.00, Eps=0.5676\n",
      "Episode 490/1000: Reward=100.00, Eps=0.5586\n",
      "Episode 500/1000: Reward=300.00, Eps=0.5495\n",
      "Episode 510/1000: Reward=250.00, Eps=0.5405\n",
      "Episode 520/1000: Reward=300.00, Eps=0.5315\n",
      "Episode 530/1000: Reward=100.00, Eps=0.5225\n",
      "Episode 540/1000: Reward=100.00, Eps=0.5135\n",
      "Episode 550/1000: Reward=150.00, Eps=0.5045\n",
      "Episode 560/1000: Reward=150.00, Eps=0.4955\n",
      "Episode 570/1000: Reward=200.00, Eps=0.4865\n",
      "Episode 580/1000: Reward=250.00, Eps=0.4775\n",
      "Episode 590/1000: Reward=100.00, Eps=0.4685\n",
      "Episode 600/1000: Reward=150.00, Eps=0.4595\n",
      "Episode 610/1000: Reward=200.00, Eps=0.4505\n",
      "Episode 620/1000: Reward=200.00, Eps=0.4414\n",
      "Episode 630/1000: Reward=100.00, Eps=0.4324\n",
      "Episode 640/1000: Reward=100.00, Eps=0.4234\n",
      "Episode 650/1000: Reward=100.00, Eps=0.4144\n",
      "Episode 660/1000: Reward=150.00, Eps=0.4054\n",
      "Episode 670/1000: Reward=200.00, Eps=0.3964\n",
      "Episode 680/1000: Reward=100.00, Eps=0.3874\n",
      "Episode 690/1000: Reward=250.00, Eps=0.3784\n",
      "Episode 700/1000: Reward=150.00, Eps=0.3694\n",
      "Episode 710/1000: Reward=200.00, Eps=0.3604\n",
      "Episode 720/1000: Reward=100.00, Eps=0.3514\n",
      "Episode 730/1000: Reward=100.00, Eps=0.3423\n",
      "Episode 740/1000: Reward=150.00, Eps=0.3333\n",
      "Episode 750/1000: Reward=50.00, Eps=0.3243\n",
      "Episode 760/1000: Reward=50.00, Eps=0.3153\n",
      "Episode 770/1000: Reward=100.00, Eps=0.3063\n",
      "Episode 780/1000: Reward=50.00, Eps=0.2973\n",
      "Episode 790/1000: Reward=50.00, Eps=0.2883\n",
      "Episode 800/1000: Reward=200.00, Eps=0.2793\n",
      "Episode 810/1000: Reward=200.00, Eps=0.2703\n",
      "Episode 820/1000: Reward=50.00, Eps=0.2613\n",
      "Episode 830/1000: Reward=50.00, Eps=0.2523\n",
      "Episode 840/1000: Reward=300.00, Eps=0.2432\n",
      "Episode 850/1000: Reward=150.00, Eps=0.2342\n",
      "Episode 860/1000: Reward=150.00, Eps=0.2252\n",
      "Episode 870/1000: Reward=300.00, Eps=0.2162\n",
      "Episode 880/1000: Reward=100.00, Eps=0.2072\n",
      "Episode 890/1000: Reward=50.00, Eps=0.1982\n",
      "Episode 900/1000: Reward=100.00, Eps=0.1892\n",
      "Episode 910/1000: Reward=150.00, Eps=0.1802\n",
      "Episode 920/1000: Reward=250.00, Eps=0.1712\n",
      "Episode 930/1000: Reward=150.00, Eps=0.1622\n",
      "Episode 940/1000: Reward=50.00, Eps=0.1532\n",
      "Episode 950/1000: Reward=50.00, Eps=0.1441\n",
      "Episode 960/1000: Reward=200.00, Eps=0.1351\n",
      "Episode 970/1000: Reward=150.00, Eps=0.1261\n",
      "Episode 980/1000: Reward=50.00, Eps=0.1171\n",
      "Episode 990/1000: Reward=150.00, Eps=0.1081\n",
      "Episode 1000/1000: Reward=200.00, Eps=0.1000\n",
      "Action distribution during training: [  6733.   6956.   6785.   7111.   6814. 101180.   6782.   6875.   6864.\n",
      "   6639.   6682.   6773.   6781.   6621.   6799.   6957.   6802.   6784.]\n",
      "Training completed. Max score ever: 550.00\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "11c3ee405fb8fe3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:37:51.288606Z",
     "start_time": "2025-11-12T22:37:51.281743Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load(Trainer._file_name_for_class(CLASS_NAME))",
   "id": "1701d95cc6b60226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SARSA agent with rules characteristics:\n",
      "w shape: (1782,)\n",
      "w norm: 3.1991591\n",
      "non-zero weights: 815\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:37:51.920764Z",
     "start_time": "2025-11-12T22:37:51.300432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\", frameskip=4)\n",
    "agent.restrict_exploration()"
   ],
   "id": "f40d266430b82729",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T22:38:58.950573Z",
     "start_time": "2025-11-12T22:37:51.926330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    actions_count = np.zeros(test_env.action_space.n, dtype=np.int32)\n",
    "    while not done:\n",
    "        state = prepare_state_categorical(state).flatten()\n",
    "        action, _ = agent.epsilon_greedy(state)\n",
    "        actions_count[action] += 1\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    test_env.render()\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    print(f'Action count during round: {actions_count}')\n",
    "    print('---------------------------')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "a9f72e2ca473c891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 50.0\n",
      "Action count during round: [  0   0   0   0   0 196   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 2: Total Reward = 50.0\n",
      "Action count during round: [  0   0   0   0   0 196   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 3: Total Reward = 50.0\n",
      "Action count during round: [  0   0   0   0   0 196   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 4: Total Reward = 50.0\n",
      "Action count during round: [  0   0   0   0   0 196   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 5: Total Reward = 50.0\n",
      "Action count during round: [  0   0   0   0   0 196   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "---------------------------\n",
      "\n",
      "Average Test Reward over 5 episodes: 50.00\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
