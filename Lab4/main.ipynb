{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:14.907741Z",
     "start_time": "2025-11-07T19:30:14.128447Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import cv2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:14.918283Z",
     "start_time": "2025-11-07T19:30:14.913964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, h=21, w=21):\n",
    "        super().__init__(env)\n",
    "        self.h, self.w = h, w\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (h, w, 3), np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = cv2.resize(obs, (self.w, self.h), interpolation=cv2.INTER_NEAREST)\n",
    "        return obs.astype(np.float32) / 255.0\n"
   ],
   "id": "a84c4cf5e7b520a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:14.926220Z",
     "start_time": "2025-11-07T19:30:14.921532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_image(frame, h=21, w=21):\n",
    "    arr = np.asarray(frame)\n",
    "    if np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = (arr * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    resized = cv2.resize(arr, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return (resized.astype(np.float32) / 255.0).flatten()\n"
   ],
   "id": "10aaa06e8987359",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.020737Z",
     "start_time": "2025-11-07T19:30:14.931094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "env = ResizeObservation(env, h=21, w=21)\n",
    "observation, info = env.reset()\n"
   ],
   "id": "7c35ab875bfe2e50",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.032178Z",
     "start_time": "2025-11-07T19:30:15.024658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"action_space:\", env.action_space)\n",
    "print(\"n actions:\", env.action_space.n)"
   ],
   "id": "19860050d8c619b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(18)\n",
      "n actions: 18\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.044533Z",
     "start_time": "2025-11-07T19:30:15.038330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    meanings = env.unwrapped.get_action_meanings()\n",
    "except Exception:\n",
    "    try:\n",
    "        meanings = env.get_action_meanings()\n",
    "    except Exception:\n",
    "        meanings = None\n",
    "\n",
    "if meanings:\n",
    "    print(\"Action index -> meaning:\")\n",
    "    for i, name in enumerate(meanings):\n",
    "        print(f\"{i}: {name}\")\n",
    "else:\n",
    "    print(\"No action meanings available from the env. Use index numbers (0..n-1).\")\n"
   ],
   "id": "17d4dbe981372894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action index -> meaning:\n",
      "0: NOOP\n",
      "1: FIRE\n",
      "2: UP\n",
      "3: RIGHT\n",
      "4: LEFT\n",
      "5: DOWN\n",
      "6: UPRIGHT\n",
      "7: UPLEFT\n",
      "8: DOWNRIGHT\n",
      "9: DOWNLEFT\n",
      "10: UPFIRE\n",
      "11: RIGHTFIRE\n",
      "12: LEFTFIRE\n",
      "13: DOWNFIRE\n",
      "14: UPRIGHTFIRE\n",
      "15: UPLEFTFIRE\n",
      "16: DOWNRIGHTFIRE\n",
      "17: DOWNLEFTFIRE\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.056988Z",
     "start_time": "2025-11-07T19:30:15.045134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIRE_ACTIONS = [1, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "MOVE_ACTIONS = [2, 3, 4, 5, 6, 7, 8, 9]"
   ],
   "id": "f2be093af0a166d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.067853Z",
     "start_time": "2025-11-07T19:30:15.064011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_frame(frame):\n",
    "    # clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    # plt.show()"
   ],
   "id": "1a691094b8caf081",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.075930Z",
     "start_time": "2025-11-07T19:30:15.071653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "id": "2473be146375f018",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.091923Z",
     "start_time": "2025-11-07T19:30:15.081605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-2\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    feature_h, feature_w = 21, 21\n",
    "    use_traces = False\n",
    "    lmbda = 0.9\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.state_dim = self.feature_h * self.feature_w * 3\n",
    "        feature_dim = self.state_dim + n_actions\n",
    "        self.w = np.zeros(feature_dim, dtype=np.float32)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        a_onehot = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        a_onehot[action] = 1.0\n",
    "        return np.concatenate([features, a_onehot])\n",
    "\n",
    "    def q_value(self, phi):\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        q_base = np.dot(state_features, self.w[:self.state_dim])\n",
    "        return q_base + self.w[self.state_dim:]\n",
    "\n",
    "    def _dimension_guard(self, features):\n",
    "        if features.shape[0] != self.feature_dim:\n",
    "            raise ValueError(f\"Expected features of dimension {self.feature_dim}, got {features.shape[0]}\")\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        eps = float(getattr(self, \"epsilon\", 0.0))\n",
    "        eps = max(0.0, min(1.0, eps))\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "        return action, q_vals\n",
    "\n",
    "    def save(self, fileName=\"sarsa_weights.npz\"):\n",
    "        np.savez(fileName, w=self.w)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_name=\"sarsa_weights.npz\"):\n",
    "        data = np.load(file_name)\n",
    "        ag = Sarsa(n_actions=data['w'].shape[0] - 21*21*3)\n",
    "        ag.w = data['w']\n",
    "\n",
    "        print(\"Loaded SARSA agent with rules characteristics:\")\n",
    "        print(\"w shape:\", ag.w.shape)\n",
    "        print(\"w norm:\", np.linalg.norm(ag.w))\n",
    "        print(\"non-zero weights:\", np.count_nonzero(ag.w))\n",
    "        return ag\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0"
   ],
   "id": "8acb4d0de5744958",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.101539Z",
     "start_time": "2025-11-07T19:30:15.097238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_model_trained():\n",
    "    try:\n",
    "        _ = np.load(\"sarsa_weights.npz\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "a2ae5147bcbeccf2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.109045Z",
     "start_time": "2025-11-07T19:30:15.104194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file_exist(file_name):\n",
    "    try:\n",
    "        _ = np.load(file_name)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "d4222e2eabe22c95",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.117821Z",
     "start_time": "2025-11-07T19:30:15.114020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainPreferences(Enum):\n",
    "    DEFAULT = 0\n",
    "    KAMIKAZE = 1"
   ],
   "id": "bb94d9156cde670d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.126515Z",
     "start_time": "2025-11-07T19:30:15.117821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScoreModifier:\n",
    "    def __init__(self, preferences=TrainPreferences.DEFAULT):\n",
    "        self.preferences = preferences\n",
    "        self.rescore_func = {\n",
    "            TrainPreferences.DEFAULT: self._default_rescore,\n",
    "            TrainPreferences.KAMIKAZE: self._kamikaze_rescore\n",
    "        }[preferences]\n",
    "\n",
    "    def rescore(self, score, action):\n",
    "        return self.rescore_func(score, action)\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_rescore(score, action):\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def _kamikaze_rescore(score, action):\n",
    "        if action == 0:\n",
    "            score -= 2\n",
    "        if action in FIRE_ACTIONS:\n",
    "            score += 1\n",
    "        if action in MOVE_ACTIONS:\n",
    "            score += 0.2\n",
    "        return score\n"
   ],
   "id": "8609bf9fe7a9ce47",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:30:15.141295Z",
     "start_time": "2025-11-07T19:30:15.128495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epsilon_min = 0.05, epsilon_decay_fraction = 0.8, initial_epsilon = 1.0):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_fraction = epsilon_decay_fraction\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_name_for_class(class_name):\n",
    "        return f\"sarsa-weights-{class_name.lower()}.npz\"\n",
    "\n",
    "    def train_if_needed(self, model, env, class_name, preferences = TrainPreferences.DEFAULT, n_episodes=1000):\n",
    "        file_name = Trainer._file_name_for_class(class_name)\n",
    "        print(f'Checking for existing model file: {file_name}')\n",
    "        if not file_exist(file_name):\n",
    "            self.train(model, env, class_name, preferences, n_episodes)\n",
    "            return model\n",
    "\n",
    "        return Sarsa.load(file_name)\n",
    "\n",
    "    def train(self, model, env, class_name, preferences = TrainPreferences.DEFAULT, n_episodes=1000):\n",
    "        print(f\"Training {class_name} agent...\")\n",
    "        score_modifier = ScoreModifier(preferences)\n",
    "\n",
    "        action_counts = np.zeros(env.action_space.n, dtype=np.float32)\n",
    "\n",
    "        model.epsilon = self.initial_epsilon\n",
    "        decay_episodes = int(n_episodes * self.epsilon_decay_fraction)\n",
    "        epsilon_decay_step = 0\n",
    "        if decay_episodes > 0:\n",
    "             epsilon_decay_step = (self.initial_epsilon - self.epsilon_min) / decay_episodes\n",
    "        else:\n",
    "             epsilon_decay_step = 0\n",
    "        print(f\"Epsilon will decay from {self.initial_epsilon} to {self.epsilon_min} over {decay_episodes} episodes.\")\n",
    "\n",
    "        log_step = max(1, n_episodes // 100)\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            features = np.array(state).flatten()\n",
    "            action, q_values = model.epsilon_greedy(features)\n",
    "            action_counts[action] += 1\n",
    "            phi = model.phi_from_state_action(features, action)\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            q_next = None\n",
    "            next_phi = None\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                next_features = np.array(next_state).flatten()\n",
    "                next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                action_counts[next_action] += 1\n",
    "                next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "                if len(q_values) != model.n_actions:\n",
    "                    raise ValueError(f\"Expected q_values of length {model.n_actions}, got {len(q_values)}\")\n",
    "\n",
    "                q = q_values[action]\n",
    "                q_next = next_q_values[next_action]\n",
    "\n",
    "                reward = score_modifier.rescore(reward, action)\n",
    "                delta = reward + model.gamma * q_next - q\n",
    "\n",
    "                model.w += model.alpha * delta * phi\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                q_values = next_q_values\n",
    "                phi = next_phi\n",
    "                ep_reward += reward\n",
    "\n",
    "            new_epsilon = model.epsilon - epsilon_decay_step\n",
    "            model.epsilon = max(self.epsilon_min, new_epsilon)\n",
    "\n",
    "            if (episode + 1) % log_step == 0:\n",
    "                print(f\"Episode {episode+1}/{n_episodes}: Reward={ep_reward:.2f}, Eps={model.epsilon:.4f}\")\n",
    "\n",
    "        print(f'Action distribution during training: {action_counts}')\n",
    "        model.save(self._file_name_for_class(class_name))\n"
   ],
   "id": "ddeb0738de17c205",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:32:15.477980Z",
     "start_time": "2025-11-07T19:30:15.148431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_NAME = \"Berzerk-Default\"\n",
    "\n",
    "agent = Sarsa(env.action_space.n)\n",
    "\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "trainer = Trainer(epsilon_min, epsilon_decay)\n",
    "agent = trainer.train_if_needed(agent, env, class_name=CLASS_NAME, preferences=TrainPreferences.DEFAULT, n_episodes=1000)\n",
    "\n",
    "env.close()\n"
   ],
   "id": "383bd49de17cecc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model file: sarsa-weights-berzerk-default.npz\n",
      "Training Berzerk-Default agent...\n",
      "Epsilon will decay from 1.0 to 0.1 over 995 episodes.\n",
      "Episode 10/1000: Reward=200.00, Eps=0.9910\n",
      "Episode 20/1000: Reward=150.00, Eps=0.9819\n",
      "Episode 30/1000: Reward=100.00, Eps=0.9729\n",
      "Episode 40/1000: Reward=0.00, Eps=0.9638\n",
      "Episode 50/1000: Reward=250.00, Eps=0.9548\n",
      "Episode 60/1000: Reward=100.00, Eps=0.9457\n",
      "Episode 70/1000: Reward=200.00, Eps=0.9367\n",
      "Episode 80/1000: Reward=50.00, Eps=0.9276\n",
      "Episode 90/1000: Reward=550.00, Eps=0.9186\n",
      "Episode 100/1000: Reward=350.00, Eps=0.9095\n",
      "Episode 110/1000: Reward=50.00, Eps=0.9005\n",
      "Episode 120/1000: Reward=150.00, Eps=0.8915\n",
      "Episode 130/1000: Reward=50.00, Eps=0.8824\n",
      "Episode 140/1000: Reward=100.00, Eps=0.8734\n",
      "Episode 150/1000: Reward=50.00, Eps=0.8643\n",
      "Episode 160/1000: Reward=50.00, Eps=0.8553\n",
      "Episode 170/1000: Reward=350.00, Eps=0.8462\n",
      "Episode 180/1000: Reward=150.00, Eps=0.8372\n",
      "Episode 190/1000: Reward=200.00, Eps=0.8281\n",
      "Episode 200/1000: Reward=0.00, Eps=0.8191\n",
      "Episode 210/1000: Reward=100.00, Eps=0.8101\n",
      "Episode 220/1000: Reward=150.00, Eps=0.8010\n",
      "Episode 230/1000: Reward=150.00, Eps=0.7920\n",
      "Episode 240/1000: Reward=200.00, Eps=0.7829\n",
      "Episode 250/1000: Reward=150.00, Eps=0.7739\n",
      "Episode 260/1000: Reward=150.00, Eps=0.7648\n",
      "Episode 270/1000: Reward=0.00, Eps=0.7558\n",
      "Episode 280/1000: Reward=200.00, Eps=0.7467\n",
      "Episode 290/1000: Reward=100.00, Eps=0.7377\n",
      "Episode 300/1000: Reward=300.00, Eps=0.7286\n",
      "Episode 310/1000: Reward=0.00, Eps=0.7196\n",
      "Episode 320/1000: Reward=200.00, Eps=0.7106\n",
      "Episode 330/1000: Reward=50.00, Eps=0.7015\n",
      "Episode 340/1000: Reward=100.00, Eps=0.6925\n",
      "Episode 350/1000: Reward=200.00, Eps=0.6834\n",
      "Episode 360/1000: Reward=150.00, Eps=0.6744\n",
      "Episode 370/1000: Reward=200.00, Eps=0.6653\n",
      "Episode 380/1000: Reward=50.00, Eps=0.6563\n",
      "Episode 390/1000: Reward=150.00, Eps=0.6472\n",
      "Episode 400/1000: Reward=0.00, Eps=0.6382\n",
      "Episode 410/1000: Reward=100.00, Eps=0.6291\n",
      "Episode 420/1000: Reward=100.00, Eps=0.6201\n",
      "Episode 430/1000: Reward=300.00, Eps=0.6111\n",
      "Episode 440/1000: Reward=50.00, Eps=0.6020\n",
      "Episode 450/1000: Reward=150.00, Eps=0.5930\n",
      "Episode 460/1000: Reward=300.00, Eps=0.5839\n",
      "Episode 470/1000: Reward=250.00, Eps=0.5749\n",
      "Episode 480/1000: Reward=100.00, Eps=0.5658\n",
      "Episode 490/1000: Reward=0.00, Eps=0.5568\n",
      "Episode 500/1000: Reward=100.00, Eps=0.5477\n",
      "Episode 510/1000: Reward=0.00, Eps=0.5387\n",
      "Episode 520/1000: Reward=100.00, Eps=0.5296\n",
      "Episode 530/1000: Reward=50.00, Eps=0.5206\n",
      "Episode 540/1000: Reward=350.00, Eps=0.5116\n",
      "Episode 550/1000: Reward=150.00, Eps=0.5025\n",
      "Episode 560/1000: Reward=50.00, Eps=0.4935\n",
      "Episode 570/1000: Reward=50.00, Eps=0.4844\n",
      "Episode 580/1000: Reward=200.00, Eps=0.4754\n",
      "Episode 590/1000: Reward=300.00, Eps=0.4663\n",
      "Episode 600/1000: Reward=100.00, Eps=0.4573\n",
      "Episode 610/1000: Reward=100.00, Eps=0.4482\n",
      "Episode 620/1000: Reward=0.00, Eps=0.4392\n",
      "Episode 630/1000: Reward=100.00, Eps=0.4302\n",
      "Episode 640/1000: Reward=450.00, Eps=0.4211\n",
      "Episode 650/1000: Reward=150.00, Eps=0.4121\n",
      "Episode 660/1000: Reward=200.00, Eps=0.4030\n",
      "Episode 670/1000: Reward=100.00, Eps=0.3940\n",
      "Episode 680/1000: Reward=0.00, Eps=0.3849\n",
      "Episode 690/1000: Reward=250.00, Eps=0.3759\n",
      "Episode 700/1000: Reward=100.00, Eps=0.3668\n",
      "Episode 710/1000: Reward=200.00, Eps=0.3578\n",
      "Episode 720/1000: Reward=100.00, Eps=0.3487\n",
      "Episode 730/1000: Reward=200.00, Eps=0.3397\n",
      "Episode 740/1000: Reward=150.00, Eps=0.3307\n",
      "Episode 750/1000: Reward=200.00, Eps=0.3216\n",
      "Episode 760/1000: Reward=250.00, Eps=0.3126\n",
      "Episode 770/1000: Reward=0.00, Eps=0.3035\n",
      "Episode 780/1000: Reward=0.00, Eps=0.2945\n",
      "Episode 790/1000: Reward=50.00, Eps=0.2854\n",
      "Episode 800/1000: Reward=0.00, Eps=0.2764\n",
      "Episode 810/1000: Reward=100.00, Eps=0.2673\n",
      "Episode 820/1000: Reward=100.00, Eps=0.2583\n",
      "Episode 830/1000: Reward=50.00, Eps=0.2492\n",
      "Episode 840/1000: Reward=300.00, Eps=0.2402\n",
      "Episode 850/1000: Reward=300.00, Eps=0.2312\n",
      "Episode 860/1000: Reward=300.00, Eps=0.2221\n",
      "Episode 870/1000: Reward=400.00, Eps=0.2131\n",
      "Episode 880/1000: Reward=50.00, Eps=0.2040\n",
      "Episode 890/1000: Reward=200.00, Eps=0.1950\n",
      "Episode 900/1000: Reward=250.00, Eps=0.1859\n",
      "Episode 910/1000: Reward=350.00, Eps=0.1769\n",
      "Episode 920/1000: Reward=100.00, Eps=0.1678\n",
      "Episode 930/1000: Reward=100.00, Eps=0.1588\n",
      "Episode 940/1000: Reward=50.00, Eps=0.1497\n",
      "Episode 950/1000: Reward=50.00, Eps=0.1407\n",
      "Episode 960/1000: Reward=0.00, Eps=0.1317\n",
      "Episode 970/1000: Reward=50.00, Eps=0.1226\n",
      "Episode 980/1000: Reward=50.00, Eps=0.1136\n",
      "Episode 990/1000: Reward=100.00, Eps=0.1045\n",
      "Episode 1000/1000: Reward=200.00, Eps=0.1000\n",
      "Action distribution during training: [22049. 10743. 10938. 22548. 10329. 12287. 10807.  9632. 12571.  9577.\n",
      " 13374. 10559.  9185. 21877. 12128. 11141. 16599. 11419.]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "dbbbe48d72337db7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:32:15.590683Z",
     "start_time": "2025-11-07T19:32:15.582116Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load(Trainer._file_name_for_class(CLASS_NAME))",
   "id": "1fb8f39f979914c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SARSA agent with rules characteristics:\n",
      "w shape: (1341,)\n",
      "w norm: 171.35416\n",
      "non-zero weights: 960\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:32:16.190645Z",
     "start_time": "2025-11-07T19:32:15.597184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\", frameskip=4)\n",
    "agent.restrict_exploration()"
   ],
   "id": "bde1bebc2b701858",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:36:26.931253Z",
     "start_time": "2025-11-07T19:32:16.199680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    actions_count = np.zeros(test_env.action_space.n, dtype=np.int32)\n",
    "    while not done:\n",
    "        state = prepare_image(state).flatten()\n",
    "        action, _ = agent.epsilon_greedy(state)\n",
    "        actions_count[action] += 1\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    test_env.render()\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    print(f'Action count during round: {actions_count}')\n",
    "    print('---------------------------')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "a6d244c1a09c42ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 550.0\n",
      "Action count during round: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 747   0   0   0]\n",
      "---------------------------\n",
      "Episode 2: Total Reward = 550.0\n",
      "Action count during round: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 747   0   0   0]\n",
      "---------------------------\n",
      "Episode 3: Total Reward = 550.0\n",
      "Action count during round: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 747   0   0   0]\n",
      "---------------------------\n",
      "Episode 4: Total Reward = 550.0\n",
      "Action count during round: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 747   0   0   0]\n",
      "---------------------------\n",
      "Episode 5: Total Reward = 550.0\n",
      "Action count during round: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 747   0   0   0]\n",
      "---------------------------\n",
      "\n",
      "Average Test Reward over 5 episodes: 550.00\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
