{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:13.617347Z",
     "start_time": "2025-11-13T13:37:13.612350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "import numba\n",
    "import plotly.express as px"
   ],
   "id": "607ff9f21f6f4806",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:13.631066Z",
     "start_time": "2025-11-13T13:37:13.625910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONST_COLOR_PLAYER = (240, 170, 103)\n",
    "CONST_COLOR_WALL = (84, 92, 214)\n",
    "CONST_COLOR_ENEMY = (210, 210, 64)\n",
    "\n",
    "CAT_EMPTY = 0\n",
    "CAT_PLAYER = 1\n",
    "CAT_WALL = 2\n",
    "CAT_ENEMY = 3"
   ],
   "id": "6e260a753c58a77",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:13.647741Z",
     "start_time": "2025-11-13T13:37:13.631066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Helper: The One-Hot Encoder (Unchanged from your code) ---\n",
    "@numba.njit\n",
    "def _get_one_hot_state(obs_resized, h, w):\n",
    "    new_obs = np.full((h, w, 4), 0, dtype=np.uint8)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            pixel = obs_resized[i, j]\n",
    "            if (pixel[0] == CONST_COLOR_PLAYER[0] and\n",
    "                pixel[1] == CONST_COLOR_PLAYER[1] and\n",
    "                pixel[2] == CONST_COLOR_PLAYER[2]):\n",
    "                new_obs[i, j, CAT_PLAYER] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_WALL[0] and\n",
    "                  pixel[1] == CONST_COLOR_WALL[1] and\n",
    "                  pixel[2] == CONST_COLOR_WALL[2]):\n",
    "                new_obs[i, j, CAT_WALL] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_ENEMY[0] and\n",
    "                  pixel[1] == CONST_COLOR_ENEMY[1] and\n",
    "                  pixel[2] == CONST_COLOR_ENEMY[2]):\n",
    "                new_obs[i, j, CAT_ENEMY] = 1\n",
    "            else:\n",
    "                new_obs[i, j, CAT_EMPTY] = 1\n",
    "    return new_obs\n",
    "\n",
    "# --- Helper: Find Entities (Copied from your StateObserver) ---\n",
    "@numba.njit\n",
    "def _find_entities(one_hot_state):\n",
    "    player_coords = np.argwhere(one_hot_state[:, :, CAT_PLAYER] == 1)\n",
    "    enemy_coords = np.argwhere(one_hot_state[:, :, CAT_ENEMY] == 1)\n",
    "    player_pos = player_coords[0] if len(player_coords) > 0 else None\n",
    "    return player_pos, enemy_coords\n",
    "\n",
    "# --- Helper: Find Closest Enemy Direction ---\n",
    "@numba.njit\n",
    "def _get_enemy_direction(player_pos, enemies):\n",
    "    if player_pos is None or len(enemies) == 0:\n",
    "        return 0.0, 0.0 # No enemy, no direction\n",
    "\n",
    "    closest_enemy = enemies[0]\n",
    "    min_dist = np.inf\n",
    "\n",
    "    # Find the closest enemy\n",
    "    for i in range(len(enemies)):\n",
    "        dist = np.sum(np.abs(enemies[i] - player_pos))\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_enemy = enemies[i]\n",
    "\n",
    "    # Calculate direction (returns -1.0, 0.0, or 1.0)\n",
    "    # We normalize to prevent one direction from seeming \"larger\"\n",
    "    dir_y = np.sign(closest_enemy[0] - player_pos[0])\n",
    "    dir_x = np.sign(closest_enemy[1] - player_pos[1])\n",
    "\n",
    "    return float(dir_y), float(dir_x)\n",
    "\n",
    "# --- THE NEW PREPROCESSING FUNCTION ---\n",
    "def extract_intelligent_features(frame, h=21, w=21):\n",
    "    \"\"\"\n",
    "    This is the new main preprocessing function.\n",
    "    It returns a 6-feature vector, not a 1764-pixel vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. First, get the raw 0-255 image\n",
    "    obs_resized = cv2.resize(frame, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 2. Get the 21x21x4 one-hot state\n",
    "    one_hot_state = _get_one_hot_state(obs_resized, h, w)\n",
    "\n",
    "    # 3. Find the player and enemies\n",
    "    player_pos, enemies = _find_entities(one_hot_state)\n",
    "\n",
    "    if player_pos is None:\n",
    "        # Player is dead, return a zero-vector\n",
    "        return np.zeros(6, dtype=np.float32)\n",
    "\n",
    "    (y, x) = player_pos\n",
    "\n",
    "    # 4. Feature 0-3: Wall Proximity\n",
    "    # (1.0 if a wall is there, 0.0 if not)\n",
    "    f_wall_up = one_hot_state[y-1, x, CAT_WALL] if y > 0 else 1.0\n",
    "    f_wall_down = one_hot_state[y+1, x, CAT_WALL] if y < h-1 else 1.0\n",
    "    f_wall_left = one_hot_state[y, x-1, CAT_WALL] if x > 0 else 1.0\n",
    "    f_wall_right = one_hot_state[y, x+1, CAT_WALL] if x < w-1 else 1.0\n",
    "\n",
    "    # 5. Feature 4-5: Enemy Direction\n",
    "    f_enemy_dir_y, f_enemy_dir_x = _get_enemy_direction(player_pos, enemies)\n",
    "\n",
    "    # 6. Return the final, intelligent feature vector\n",
    "    features = np.array([\n",
    "        f_wall_up, f_wall_down, f_wall_left, f_wall_right,\n",
    "        f_enemy_dir_y, f_enemy_dir_x\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return features"
   ],
   "id": "db176227558f2a08",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:13.656918Z",
     "start_time": "2025-11-13T13:37:13.651634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, h=21, w=21):\n",
    "        super().__init__(env)\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "        # --- NEW OBSERVATION SPACE ---\n",
    "        # 6 features: wall_up, wall_down, wall_left, wall_right,\n",
    "        #             enemy_dir_y, enemy_dir_x\n",
    "        # Values are -1.0 to 1.0 (for directions and wall flags)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(6,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Call our new, intelligent feature extractor\n",
    "        return extract_intelligent_features(obs, self.h, self.w)"
   ],
   "id": "c0da2a556648f73a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.331268Z",
     "start_time": "2025-11-13T13:37:13.661388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "env = ResizeObservation(env, h=21, w=21)\n",
    "observation, info = env.reset()\n"
   ],
   "id": "88ee4fa8a356cbef",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.340538Z",
     "start_time": "2025-11-13T13:37:14.335318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"action_space:\", env.action_space)\n",
    "print(\"n actions:\", env.action_space.n)"
   ],
   "id": "c549efd9095a5b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(18)\n",
      "n actions: 18\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.355958Z",
     "start_time": "2025-11-13T13:37:14.349690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    meanings = env.unwrapped.get_action_meanings()\n",
    "except Exception:\n",
    "    try:\n",
    "        meanings = env.get_action_meanings()\n",
    "    except Exception:\n",
    "        meanings = None\n",
    "\n",
    "if meanings:\n",
    "    print(\"Action index -> meaning:\")\n",
    "    for i, name in enumerate(meanings):\n",
    "        print(f\"{i}: {name}\")\n",
    "else:\n",
    "    print(\"No action meanings available from the env. Use index numbers (0..n-1).\")\n"
   ],
   "id": "5f62493b12cd715e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action index -> meaning:\n",
      "0: NOOP\n",
      "1: FIRE\n",
      "2: UP\n",
      "3: RIGHT\n",
      "4: LEFT\n",
      "5: DOWN\n",
      "6: UPRIGHT\n",
      "7: UPLEFT\n",
      "8: DOWNRIGHT\n",
      "9: DOWNLEFT\n",
      "10: UPFIRE\n",
      "11: RIGHTFIRE\n",
      "12: LEFTFIRE\n",
      "13: DOWNFIRE\n",
      "14: UPRIGHTFIRE\n",
      "15: UPLEFTFIRE\n",
      "16: DOWNRIGHTFIRE\n",
      "17: DOWNLEFTFIRE\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.369386Z",
     "start_time": "2025-11-13T13:37:14.364492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIRE_ACTIONS = [1, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "MOVE_ACTIONS = [2, 3, 4, 5, 6, 7, 8, 9]"
   ],
   "id": "893650ccbe022de3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.382402Z",
     "start_time": "2025-11-13T13:37:14.375928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "id": "c49df2e1ad97e294",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.396534Z",
     "start_time": "2025-11-13T13:37:14.387395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-4\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    # feature_h, feature_w = 21, 21\n",
    "    lmbda = 0.9\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.state_dim = 6\n",
    "        feature_dim = self.state_dim + n_actions\n",
    "        self.w = np.zeros(feature_dim, dtype=np.float32)\n",
    "        self.n_actions = n_actions\n",
    "        self.z = np.zeros_like(self.w, dtype=np.float32)\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        a_onehot = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        a_onehot[action] = 1.0\n",
    "        return np.concatenate([features, a_onehot])\n",
    "\n",
    "    def q_value(self, phi):\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        q_base = np.dot(state_features, self.w[:self.state_dim])\n",
    "        return q_base + self.w[self.state_dim:]\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        eps = float(getattr(self, \"epsilon\", 0.0))\n",
    "        eps = max(0.0, min(1.0, eps))\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "        return action, q_vals\n",
    "\n",
    "    def save(self, file_name=\"sarsa_weights.npz\"):\n",
    "        np.savez(file_name, w=self.w)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_name=\"sarsa_weights.npz\"):\n",
    "        data = np.load(file_name)\n",
    "        n_featues = 6\n",
    "        ag = Sarsa(n_actions=data['w'].shape[0] - n_featues)\n",
    "        ag.w = data['w']\n",
    "\n",
    "        print(\"Loaded SARSA agent with rules characteristics:\")\n",
    "        print(\"w shape:\", ag.w.shape)\n",
    "        print(\"w norm:\", np.linalg.norm(ag.w))\n",
    "        print(\"non-zero weights:\", np.count_nonzero(ag.w))\n",
    "        return ag\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def reset_traces(self):\n",
    "        self.z.fill(0.0)"
   ],
   "id": "23f7d9eb90e736ca",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.404729Z",
     "start_time": "2025-11-13T13:37:14.400193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_model_trained():\n",
    "    try:\n",
    "        _ = np.load(\"sarsa_weights.npz\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "3ea75ff7bcd035e6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.413170Z",
     "start_time": "2025-11-13T13:37:14.409408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file_exist(file_name):\n",
    "    try:\n",
    "        _ = np.load(file_name)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "342b5ae0c6a0411e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.425500Z",
     "start_time": "2025-11-13T13:37:14.413170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_LIVING = -0.001            # MANUAL: Must be a small negative \"living penalty\".\n",
    "R_INACTION = -0.2603       # IRL: Learned from your data.\n",
    "R_WALL = -2.082           # IRL: Learned from your data.\n",
    "# R_INACTION = -1.082       # IRL: Learned from your data.\n",
    "# R_WALL = -1.082           # IRL: Learned from your data.\n",
    "R_PROXIMITY = -0.0499      # IRL: Learned from your data.\n",
    "R_HUNTING = 0.2            # MANUAL: Override. The model failed to learn this.\n",
    "R_KILL = 15.0              # MANUAL: Override. The model failed to learn this.\n",
    "R_DEATH = -20.0            # MANUAL: Override. The model's value was too small.\n",
    "\n",
    "class StateObserver:\n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.prev_pos = None\n",
    "        self.prev_num_enemies = 0\n",
    "        self.prev_min_distance = None\n",
    "\n",
    "    def _find_entities(self, state):\n",
    "        player_coords = np.argwhere(state[:, :, CAT_PLAYER] == 1)\n",
    "        enemy_coords = np.argwhere(state[:, :, CAT_ENEMY] == 1)\n",
    "        player_pos = tuple(player_coords[0]) if len(player_coords) > 0 else None\n",
    "        return player_pos, enemy_coords\n",
    "\n",
    "    def analyze_state(self, state, reward, action):\n",
    "        player_pos, enemies = self._find_entities(state)\n",
    "        num_enemies = len(enemies)\n",
    "\n",
    "        # Start with the real reward from the game\n",
    "        shaped_reward = reward\n",
    "\n",
    "        # 1. Living Penalty: Small cost for every step to encourage speed.\n",
    "        shaped_reward += R_LIVING\n",
    "\n",
    "        # if action not in MOVE_ACTIONS:\n",
    "        #     shaped_reward += R_INACTION\n",
    "\n",
    "        if (action in MOVE_ACTIONS and\n",
    "            player_pos is not None and\n",
    "            player_pos == self.prev_pos):\n",
    "            shaped_reward += R_WALL\n",
    "\n",
    "        if num_enemies < self.prev_num_enemies:\n",
    "            shaped_reward += R_KILL\n",
    "\n",
    "        # 4. Proximity Penalty: Penalize being too close to an enemy.\n",
    "        if player_pos is not None and num_enemies > 0:\n",
    "            distances = np.sum(np.abs(enemies - player_pos), axis=1)\n",
    "            min_distance = np.min(distances)\n",
    "\n",
    "            if min_distance <= 2: # Too close!\n",
    "                shaped_reward += R_PROXIMITY\n",
    "\n",
    "            if self.prev_min_distance is not None and min_distance < self.prev_min_distance:\n",
    "                shaped_reward += R_HUNTING\n",
    "\n",
    "            self.prev_min_distance = min_distance\n",
    "        else:\n",
    "            self.prev_min_distance = None\n",
    "\n",
    "        # 5. Death Penalty: Big penalty if player disappears.\n",
    "        if player_pos is None and self.prev_pos is not None:\n",
    "             # Player was alive last step, but is gone now\n",
    "             shaped_reward += R_DEATH\n",
    "\n",
    "        # Update memory for the next step\n",
    "        self.prev_pos = player_pos\n",
    "        self.prev_num_enemies = num_enemies\n",
    "        return shaped_reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.prev_pos = None\n",
    "        self.prev_num_enemies = 0\n",
    "        self.prev_min_distance = None\n"
   ],
   "id": "19f739f20f96bb61",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.448071Z",
     "start_time": "2025-11-13T13:37:14.431330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epsilon_min = 0.05, epsilon_decay_fraction = 0.999, initial_epsilon = 1.0):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_fraction = epsilon_decay_fraction\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_name_for_class(class_name):\n",
    "        return f\"sarsa-weights-{class_name.lower()}.npz\"\n",
    "\n",
    "    def train_if_needed(self, model, env, class_name, n_episodes=1000):\n",
    "        file_name = Trainer._file_name_for_class(class_name)\n",
    "        print(f'Checking for existing model file: {file_name}')\n",
    "        if not file_exist(file_name):\n",
    "            self.train(model, env, class_name, n_episodes)\n",
    "            return model\n",
    "\n",
    "        return Sarsa.load(file_name)\n",
    "\n",
    "    def train(self, model, env, class_name, n_episodes=1000):\n",
    "        print(f\"Training {class_name} agent...\")\n",
    "        state_observer = StateObserver(w=21, h=21)\n",
    "        action_counts = np.zeros(env.action_space.n, dtype=np.float32)\n",
    "\n",
    "        max_score_ever = -np.inf\n",
    "        rewards = []\n",
    "        w_changes = []\n",
    "        previous_w = model.w.copy()\n",
    "\n",
    "        model.epsilon = self.initial_epsilon\n",
    "        decay_episodes = int(n_episodes * self.epsilon_decay_fraction)\n",
    "        if decay_episodes > 0:\n",
    "             epsilon_decay_step = (self.initial_epsilon - self.epsilon_min) / decay_episodes\n",
    "        else:\n",
    "             epsilon_decay_step = 0\n",
    "        print(f\"Epsilon will decay from {self.initial_epsilon} to {self.epsilon_min} over {decay_episodes} episodes.\")\n",
    "\n",
    "        log_step = max(1, n_episodes // 100)\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state_observer.reset()\n",
    "            model.reset_traces()\n",
    "            features = np.array(state, dtype=np.float32)\n",
    "            action, q_values = model.epsilon_greedy(features)\n",
    "            action_counts[action] += 1\n",
    "            phi = model.phi_from_state_action(features, action)\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Ensure next_state is properly flattened\n",
    "                next_features = np.array(next_state, dtype=np.float32)\n",
    "                next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                action_counts[next_action] += 1\n",
    "                next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "                if len(q_values) != model.n_actions:\n",
    "                    raise ValueError(f\"Expected q_values of length {model.n_actions}, got {len(q_values)}\")\n",
    "\n",
    "                q = q_values[action]\n",
    "                if done:\n",
    "                    q_next = 0.0\n",
    "                    continue\n",
    "                else:\n",
    "                    # next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                    q_next = next_q_values[next_action]\n",
    "\n",
    "                    # Store these for the *next* loop iteration\n",
    "\n",
    "                action_counts[next_action] += 1\n",
    "                # next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "\n",
    "                # Use the original state (2D array) for state observer\n",
    "                shaped_reward = reward + R_LIVING\n",
    "                # if action not in MOVE_ACTIONS:\n",
    "                #     shaped_reward += R_INACTION\n",
    "\n",
    "                if action == 2: # UP\n",
    "                    shaped_reward += next_features[0] * R_WALL\n",
    "                elif action == 5: # DOWN\n",
    "                    shaped_reward += next_features[1] * R_WALL # f_wall_down\n",
    "                elif action == 4: # LEFT\n",
    "                    shaped_reward += next_features[2] * R_WALL # f_wall_left\n",
    "                elif action == 3: # RIGHT\n",
    "                    shaped_reward += next_features[3] * R_WALL\n",
    "\n",
    "                if reward > 0:\n",
    "                     shaped_reward += R_KILL\n",
    "\n",
    "                # 4. Hunting Reward\n",
    "                f_enemy_dir_y, f_enemy_dir_x = features[4], features[5]\n",
    "                if f_enemy_dir_y != 0 or f_enemy_dir_x != 0: # If an enemy exists\n",
    "                    if (action == 2 and f_enemy_dir_y == -1.0) or \\\n",
    "                       (action == 5 and f_enemy_dir_y == 1.0) or \\\n",
    "                       (action == 4 and f_enemy_dir_x == -1.0) or \\\n",
    "                       (action == 3 and f_enemy_dir_x == 1.0):\n",
    "                        shaped_reward += R_HUNTING\n",
    "\n",
    "                if terminated:\n",
    "                    shaped_reward += R_DEATH\n",
    "\n",
    "                delta = shaped_reward + model.gamma * q_next - q\n",
    "                model.z = (model.gamma * model.lmbda * model.z) + phi\n",
    "                model.w += model.alpha * delta * model.z\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                q_values = next_q_values\n",
    "                phi = next_phi\n",
    "                ep_reward += reward\n",
    "\n",
    "            new_epsilon = model.epsilon - epsilon_decay_step\n",
    "            model.epsilon = max(self.epsilon_min, new_epsilon)\n",
    "\n",
    "            w_change = np.mean(np.abs(model.w - previous_w))\n",
    "            w_changes.append(w_change)\n",
    "            previous_w = model.w.copy()\n",
    "\n",
    "            rewards.append(ep_reward)\n",
    "            if ep_reward > max_score_ever:\n",
    "                max_score_ever = ep_reward\n",
    "\n",
    "\n",
    "            if (episode + 1) % log_step == 0:\n",
    "                recent_max = float(np.max(rewards[-log_step:])) if len(rewards) > 0 else float(ep_reward)\n",
    "                print(f\"Episode {episode+1}/{n_episodes}: Max reward for period={recent_max:.2f}, Eps={model.epsilon:.4f}\")\n",
    "\n",
    "        px.line(x=np.arange(1, n_episodes + 1), y=w_changes, labels={'x': 'Episode', 'y': 'Mean |Δw|'},\n",
    "                title='Mean Weight Change over Episodes').show()\n",
    "        print(f'Action distribution during training: {action_counts}')\n",
    "        print(f\"Training completed. Max score ever: {max_score_ever:.2f}\")\n",
    "        model.save(self._file_name_for_class(class_name))\n"
   ],
   "id": "f185cf8af1717cf",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:37:14.455626Z",
     "start_time": "2025-11-13T13:37:14.451874Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f1e451f2714ca31e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:40:54.860244Z",
     "start_time": "2025-11-13T13:37:14.459782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_NAME = \"Berzerk-Default\"\n",
    "\n",
    "agent = Sarsa(env.action_space.n)\n",
    "\n",
    "epsilon_min = 0.1\n",
    "\n",
    "trainer = Trainer(epsilon_min, 0.8, initial_epsilon=0.5)\n",
    "agent = trainer.train_if_needed(agent, env, class_name=CLASS_NAME, n_episodes=1000)\n",
    "\n",
    "env.close()\n"
   ],
   "id": "e0f035ed87e04e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model file: sarsa-weights-berzerk-default.npz\n",
      "Training Berzerk-Default agent...\n",
      "Epsilon will decay from 0.5 to 0.1 over 800 episodes.\n",
      "Episode 10/1000: Max reward for period=500.00, Eps=0.4950\n",
      "Episode 20/1000: Max reward for period=450.00, Eps=0.4900\n",
      "Episode 30/1000: Max reward for period=300.00, Eps=0.4850\n",
      "Episode 40/1000: Max reward for period=500.00, Eps=0.4800\n",
      "Episode 50/1000: Max reward for period=400.00, Eps=0.4750\n",
      "Episode 60/1000: Max reward for period=500.00, Eps=0.4700\n",
      "Episode 70/1000: Max reward for period=400.00, Eps=0.4650\n",
      "Episode 80/1000: Max reward for period=450.00, Eps=0.4600\n",
      "Episode 90/1000: Max reward for period=400.00, Eps=0.4550\n",
      "Episode 100/1000: Max reward for period=450.00, Eps=0.4500\n",
      "Episode 110/1000: Max reward for period=450.00, Eps=0.4450\n",
      "Episode 120/1000: Max reward for period=450.00, Eps=0.4400\n",
      "Episode 130/1000: Max reward for period=350.00, Eps=0.4350\n",
      "Episode 140/1000: Max reward for period=350.00, Eps=0.4300\n",
      "Episode 150/1000: Max reward for period=300.00, Eps=0.4250\n",
      "Episode 160/1000: Max reward for period=400.00, Eps=0.4200\n",
      "Episode 170/1000: Max reward for period=300.00, Eps=0.4150\n",
      "Episode 180/1000: Max reward for period=350.00, Eps=0.4100\n",
      "Episode 190/1000: Max reward for period=450.00, Eps=0.4050\n",
      "Episode 200/1000: Max reward for period=650.00, Eps=0.4000\n",
      "Episode 210/1000: Max reward for period=300.00, Eps=0.3950\n",
      "Episode 220/1000: Max reward for period=500.00, Eps=0.3900\n",
      "Episode 230/1000: Max reward for period=600.00, Eps=0.3850\n",
      "Episode 240/1000: Max reward for period=250.00, Eps=0.3800\n",
      "Episode 250/1000: Max reward for period=500.00, Eps=0.3750\n",
      "Episode 260/1000: Max reward for period=400.00, Eps=0.3700\n",
      "Episode 270/1000: Max reward for period=350.00, Eps=0.3650\n",
      "Episode 280/1000: Max reward for period=250.00, Eps=0.3600\n",
      "Episode 290/1000: Max reward for period=450.00, Eps=0.3550\n",
      "Episode 300/1000: Max reward for period=350.00, Eps=0.3500\n",
      "Episode 310/1000: Max reward for period=350.00, Eps=0.3450\n",
      "Episode 320/1000: Max reward for period=450.00, Eps=0.3400\n",
      "Episode 330/1000: Max reward for period=550.00, Eps=0.3350\n",
      "Episode 340/1000: Max reward for period=350.00, Eps=0.3300\n",
      "Episode 350/1000: Max reward for period=400.00, Eps=0.3250\n",
      "Episode 360/1000: Max reward for period=350.00, Eps=0.3200\n",
      "Episode 370/1000: Max reward for period=250.00, Eps=0.3150\n",
      "Episode 380/1000: Max reward for period=300.00, Eps=0.3100\n",
      "Episode 390/1000: Max reward for period=350.00, Eps=0.3050\n",
      "Episode 400/1000: Max reward for period=450.00, Eps=0.3000\n",
      "Episode 410/1000: Max reward for period=250.00, Eps=0.2950\n",
      "Episode 420/1000: Max reward for period=600.00, Eps=0.2900\n",
      "Episode 430/1000: Max reward for period=450.00, Eps=0.2850\n",
      "Episode 440/1000: Max reward for period=500.00, Eps=0.2800\n",
      "Episode 450/1000: Max reward for period=700.00, Eps=0.2750\n",
      "Episode 460/1000: Max reward for period=350.00, Eps=0.2700\n",
      "Episode 470/1000: Max reward for period=540.00, Eps=0.2650\n",
      "Episode 480/1000: Max reward for period=550.00, Eps=0.2600\n",
      "Episode 490/1000: Max reward for period=300.00, Eps=0.2550\n",
      "Episode 500/1000: Max reward for period=500.00, Eps=0.2500\n",
      "Episode 510/1000: Max reward for period=350.00, Eps=0.2450\n",
      "Episode 520/1000: Max reward for period=300.00, Eps=0.2400\n",
      "Episode 530/1000: Max reward for period=400.00, Eps=0.2350\n",
      "Episode 540/1000: Max reward for period=500.00, Eps=0.2300\n",
      "Episode 550/1000: Max reward for period=450.00, Eps=0.2250\n",
      "Episode 560/1000: Max reward for period=450.00, Eps=0.2200\n",
      "Episode 570/1000: Max reward for period=500.00, Eps=0.2150\n",
      "Episode 580/1000: Max reward for period=450.00, Eps=0.2100\n",
      "Episode 590/1000: Max reward for period=500.00, Eps=0.2050\n",
      "Episode 600/1000: Max reward for period=500.00, Eps=0.2000\n",
      "Episode 610/1000: Max reward for period=400.00, Eps=0.1950\n",
      "Episode 620/1000: Max reward for period=450.00, Eps=0.1900\n",
      "Episode 630/1000: Max reward for period=650.00, Eps=0.1850\n",
      "Episode 640/1000: Max reward for period=450.00, Eps=0.1800\n",
      "Episode 650/1000: Max reward for period=450.00, Eps=0.1750\n",
      "Episode 660/1000: Max reward for period=550.00, Eps=0.1700\n",
      "Episode 670/1000: Max reward for period=400.00, Eps=0.1650\n",
      "Episode 680/1000: Max reward for period=550.00, Eps=0.1600\n",
      "Episode 690/1000: Max reward for period=550.00, Eps=0.1550\n",
      "Episode 700/1000: Max reward for period=350.00, Eps=0.1500\n",
      "Episode 710/1000: Max reward for period=650.00, Eps=0.1450\n",
      "Episode 720/1000: Max reward for period=500.00, Eps=0.1400\n",
      "Episode 730/1000: Max reward for period=450.00, Eps=0.1350\n",
      "Episode 740/1000: Max reward for period=550.00, Eps=0.1300\n",
      "Episode 750/1000: Max reward for period=400.00, Eps=0.1250\n",
      "Episode 760/1000: Max reward for period=550.00, Eps=0.1200\n",
      "Episode 770/1000: Max reward for period=400.00, Eps=0.1150\n",
      "Episode 780/1000: Max reward for period=550.00, Eps=0.1100\n",
      "Episode 790/1000: Max reward for period=650.00, Eps=0.1050\n",
      "Episode 800/1000: Max reward for period=400.00, Eps=0.1000\n",
      "Episode 810/1000: Max reward for period=600.00, Eps=0.1000\n",
      "Episode 820/1000: Max reward for period=650.00, Eps=0.1000\n",
      "Episode 830/1000: Max reward for period=650.00, Eps=0.1000\n",
      "Episode 840/1000: Max reward for period=450.00, Eps=0.1000\n",
      "Episode 850/1000: Max reward for period=650.00, Eps=0.1000\n",
      "Episode 860/1000: Max reward for period=750.00, Eps=0.1000\n",
      "Episode 870/1000: Max reward for period=550.00, Eps=0.1000\n",
      "Episode 880/1000: Max reward for period=650.00, Eps=0.1000\n",
      "Episode 890/1000: Max reward for period=550.00, Eps=0.1000\n",
      "Episode 900/1000: Max reward for period=600.00, Eps=0.1000\n",
      "Episode 910/1000: Max reward for period=600.00, Eps=0.1000\n",
      "Episode 920/1000: Max reward for period=450.00, Eps=0.1000\n",
      "Episode 930/1000: Max reward for period=500.00, Eps=0.1000\n",
      "Episode 940/1000: Max reward for period=600.00, Eps=0.1000\n",
      "Episode 950/1000: Max reward for period=600.00, Eps=0.1000\n",
      "Episode 960/1000: Max reward for period=500.00, Eps=0.1000\n",
      "Episode 970/1000: Max reward for period=550.00, Eps=0.1000\n",
      "Episode 980/1000: Max reward for period=500.00, Eps=0.1000\n",
      "Episode 990/1000: Max reward for period=450.00, Eps=0.1000\n",
      "Episode 1000/1000: Max reward for period=400.00, Eps=0.1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "hovertemplate": "Episode=%{x}<br>Mean |Δw|=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "x": {
          "dtype": "i2",
          "bdata": "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"
         },
         "xaxis": "x",
         "y": {
          "dtype": "f4",
          "bdata": "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"
         },
         "yaxis": "y",
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "bgcolor": "rgb(17,17,17)",
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "rgb(17,17,17)",
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "subunitcolor": "#506784",
           "showland": true,
           "showlakes": true,
           "lakecolor": "rgb(17,17,17)"
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "borderwidth": 1,
           "bordercolor": "rgb(17,17,17)",
           "tickwidth": 0
          },
          "mapbox": {
           "style": "dark"
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0.0,
          1.0
         ],
         "title": {
          "text": "Episode"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.0,
          1.0
         ],
         "title": {
          "text": "Mean |Δw|"
         }
        },
        "legend": {
         "tracegroupgap": 0
        },
        "title": {
         "text": "Mean Weight Change over Episodes"
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action distribution during training: [ 10379.  10666.  10370.  10633.  10406.  10302.  10293.  10511.  10725.\n",
      "  10488.  10421.  10465.  10396. 639596.  10515.  10735.  10476.  10617.]\n",
      "Training completed. Max score ever: 750.00\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "11c3ee405fb8fe3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:40:54.911372Z",
     "start_time": "2025-11-13T13:40:54.904851Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load(Trainer._file_name_for_class(CLASS_NAME))",
   "id": "1701d95cc6b60226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SARSA agent with rules characteristics:\n",
      "w shape: (24,)\n",
      "w norm: 53.34482\n",
      "non-zero weights: 24\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:40:55.574743Z",
     "start_time": "2025-11-13T13:40:54.940649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\", frameskip=4)\n",
    "agent.restrict_exploration()"
   ],
   "id": "f40d266430b82729",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:40:55.588169Z",
     "start_time": "2025-11-13T13:40:55.584266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_weights(w):\n",
    "    avg = np.mean(w)\n",
    "    irq = np.percentile(w, 75) - np.percentile(w, 25)\n",
    "    lower_bound = avg - irq * 0.75\n",
    "    upper_bound = avg + irq * 0.75\n",
    "    w_clipped = np.clip(w, lower_bound, upper_bound)\n",
    "    return w_clipped\n"
   ],
   "id": "5b0a81ab504daa23",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T13:40:55.624837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "agent.w = normalize_weights(agent.w)\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    actions_count = np.zeros(test_env.action_space.n, dtype=np.int32)\n",
    "    while not done:\n",
    "        features = extract_intelligent_features(state)\n",
    "        action, _ = agent.epsilon_greedy(features)\n",
    "        actions_count[action] += 1\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    test_env.render()\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    print(f'Action count during round: {actions_count}')\n",
    "    print('---------------------------')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "a9f72e2ca473c891",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
