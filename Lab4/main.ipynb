{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:46.167907Z",
     "start_time": "2025-11-13T22:37:45.026064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "import numba\n",
    "import plotly.express as px"
   ],
   "id": "607ff9f21f6f4806",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:46.177732Z",
     "start_time": "2025-11-13T22:37:46.173269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONST_COLOR_PLAYER = (240, 170, 103)\n",
    "CONST_COLOR_WALL = (84, 92, 214)\n",
    "CONST_COLOR_ENEMY = (210, 210, 64)\n",
    "\n",
    "CAT_EMPTY = 0\n",
    "CAT_PLAYER = 1\n",
    "CAT_WALL = 2\n",
    "CAT_ENEMY = 3"
   ],
   "id": "6e260a753c58a77",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:46.197988Z",
     "start_time": "2025-11-13T22:37:46.183285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Helper: The One-Hot Encoder (Unchanged from your code) ---\n",
    "@numba.njit\n",
    "def _get_one_hot_state(obs_resized, h, w):\n",
    "    new_obs = np.full((h, w, 4), 0, dtype=np.uint8)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            pixel = obs_resized[i, j]\n",
    "            if (pixel[0] == CONST_COLOR_PLAYER[0] and\n",
    "                pixel[1] == CONST_COLOR_PLAYER[1] and\n",
    "                pixel[2] == CONST_COLOR_PLAYER[2]):\n",
    "                new_obs[i, j, CAT_PLAYER] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_WALL[0] and\n",
    "                  pixel[1] == CONST_COLOR_WALL[1] and\n",
    "                  pixel[2] == CONST_COLOR_WALL[2]):\n",
    "                new_obs[i, j, CAT_WALL] = 1\n",
    "            elif (pixel[0] == CONST_COLOR_ENEMY[0] and\n",
    "                  pixel[1] == CONST_COLOR_ENEMY[1] and\n",
    "                  pixel[2] == CONST_COLOR_ENEMY[2]):\n",
    "                new_obs[i, j, CAT_ENEMY] = 1\n",
    "            else:\n",
    "                new_obs[i, j, CAT_EMPTY] = 1\n",
    "    return new_obs\n",
    "\n",
    "# --- Helper: Find Entities (Copied from your StateObserver) ---\n",
    "@numba.njit\n",
    "def _find_entities(one_hot_state):\n",
    "    player_coords = np.argwhere(one_hot_state[:, :, CAT_PLAYER] == 1)\n",
    "    enemy_coords = np.argwhere(one_hot_state[:, :, CAT_ENEMY] == 1)\n",
    "    player_pos = player_coords[0] if len(player_coords) > 0 else None\n",
    "    return player_pos, enemy_coords\n",
    "\n",
    "# --- Helper: Find Closest Enemy Direction ---\n",
    "@numba.njit\n",
    "def _get_enemy_direction(player_pos, enemies):\n",
    "    if player_pos is None or len(enemies) == 0:\n",
    "        return 0.0, 0.0 # No enemy, no direction\n",
    "\n",
    "    closest_enemy = enemies[0]\n",
    "    min_dist = np.inf\n",
    "\n",
    "    # Find the closest enemy\n",
    "    for i in range(len(enemies)):\n",
    "        dist = np.sum(np.abs(enemies[i] - player_pos))\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_enemy = enemies[i]\n",
    "\n",
    "    # Calculate direction (returns -1.0, 0.0, or 1.0)\n",
    "    # We normalize to prevent one direction from seeming \"larger\"\n",
    "    dir_y = np.sign(closest_enemy[0] - player_pos[0])\n",
    "    dir_x = np.sign(closest_enemy[1] - player_pos[1])\n",
    "\n",
    "    return float(dir_y), float(dir_x)\n",
    "\n",
    "# --- THE NEW PREPROCESSING FUNCTION ---\n",
    "def extract_intelligent_features(frame, h=21, w=21):\n",
    "    \"\"\"\n",
    "    This is the new main preprocessing function.\n",
    "    It returns a 6-feature vector, not a 1764-pixel vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. First, get the raw 0-255 image\n",
    "    obs_resized = cv2.resize(frame, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 2. Get the 21x21x4 one-hot state\n",
    "    one_hot_state = _get_one_hot_state(obs_resized, h, w)\n",
    "\n",
    "    # 3. Find the player and enemies\n",
    "    player_pos, enemies = _find_entities(one_hot_state)\n",
    "\n",
    "    if player_pos is None:\n",
    "        # Player is dead, return a zero-vector\n",
    "        return np.zeros(6, dtype=np.float32)\n",
    "\n",
    "    (y, x) = player_pos\n",
    "\n",
    "    # 4. Feature 0-3: Wall Proximity\n",
    "    # (1.0 if a wall is there, 0.0 if not)\n",
    "    f_wall_up = one_hot_state[y-1, x, CAT_WALL] if y > 0 else 1.0\n",
    "    f_wall_down = one_hot_state[y+1, x, CAT_WALL] if y < h-1 else 1.0\n",
    "    f_wall_left = one_hot_state[y, x-1, CAT_WALL] if x > 0 else 1.0\n",
    "    f_wall_right = one_hot_state[y, x+1, CAT_WALL] if x < w-1 else 1.0\n",
    "\n",
    "    # 5. Feature 4-5: Enemy Direction\n",
    "    f_enemy_dir_y, f_enemy_dir_x = _get_enemy_direction(player_pos, enemies)\n",
    "\n",
    "    # 6. Return the final, intelligent feature vector\n",
    "    features = np.array([\n",
    "        f_wall_up, f_wall_down, f_wall_left, f_wall_right,\n",
    "        f_enemy_dir_y, f_enemy_dir_x\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return features"
   ],
   "id": "db176227558f2a08",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:46.207801Z",
     "start_time": "2025-11-13T22:37:46.203521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, h=21, w=21):\n",
    "        super().__init__(env)\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "        # --- NEW OBSERVATION SPACE ---\n",
    "        # 6 features: wall_up, wall_down, wall_left, wall_right,\n",
    "        #             enemy_dir_y, enemy_dir_x\n",
    "        # Values are -1.0 to 1.0 (for directions and wall flags)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(6,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Call our new, intelligent feature extractor\n",
    "        return extract_intelligent_features(obs, self.h, self.w)"
   ],
   "id": "c0da2a556648f73a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:47.863768Z",
     "start_time": "2025-11-13T22:37:46.213320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "env = ResizeObservation(env, h=21, w=21)\n",
    "observation, info = env.reset()\n"
   ],
   "id": "88ee4fa8a356cbef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:47.962219Z",
     "start_time": "2025-11-13T22:37:47.958105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"action_space:\", env.action_space)\n",
    "print(\"n actions:\", env.action_space.n)"
   ],
   "id": "c549efd9095a5b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(18)\n",
      "n actions: 18\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.055802Z",
     "start_time": "2025-11-13T22:37:47.979767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    meanings = env.unwrapped.get_action_meanings()\n",
    "except Exception:\n",
    "    try:\n",
    "        meanings = env.get_action_meanings()\n",
    "    except Exception:\n",
    "        meanings = None\n",
    "\n",
    "if meanings:\n",
    "    print(\"Action index -> meaning:\")\n",
    "    for i, name in enumerate(meanings):\n",
    "        print(f\"{i}: {name}\")\n",
    "else:\n",
    "    print(\"No action meanings available from the env. Use index numbers (0..n-1).\")\n"
   ],
   "id": "5f62493b12cd715e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action index -> meaning:\n",
      "0: NOOP\n",
      "1: FIRE\n",
      "2: UP\n",
      "3: RIGHT\n",
      "4: LEFT\n",
      "5: DOWN\n",
      "6: UPRIGHT\n",
      "7: UPLEFT\n",
      "8: DOWNRIGHT\n",
      "9: DOWNLEFT\n",
      "10: UPFIRE\n",
      "11: RIGHTFIRE\n",
      "12: LEFTFIRE\n",
      "13: DOWNFIRE\n",
      "14: UPRIGHTFIRE\n",
      "15: UPLEFTFIRE\n",
      "16: DOWNRIGHTFIRE\n",
      "17: DOWNLEFTFIRE\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.120440Z",
     "start_time": "2025-11-13T22:37:48.115981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIRE_ACTIONS = [1, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "MOVE_ACTIONS = [2, 3, 4, 5, 6, 7, 8, 9]"
   ],
   "id": "893650ccbe022de3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.135735Z",
     "start_time": "2025-11-13T22:37:48.125994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "id": "c49df2e1ad97e294",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.149191Z",
     "start_time": "2025-11-13T22:37:48.140839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-4\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    # feature_h, feature_w = 21, 21\n",
    "    lmbda = 0.9\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.state_dim = 6\n",
    "        feature_dim = self.state_dim + n_actions\n",
    "        self.w = np.zeros(feature_dim, dtype=np.float32)\n",
    "        self.n_actions = n_actions\n",
    "        self.z = np.zeros_like(self.w, dtype=np.float32)\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        a_onehot = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        a_onehot[action] = 1.0\n",
    "        return np.concatenate([features, a_onehot])\n",
    "\n",
    "    def q_value(self, phi):\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        q_base = np.dot(state_features, self.w[:self.state_dim])\n",
    "        return q_base + self.w[self.state_dim:]\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        eps = float(getattr(self, \"epsilon\", 0.0))\n",
    "        eps = max(0.0, min(1.0, eps))\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "        return action, q_vals\n",
    "\n",
    "    def save(self, file_name=\"sarsa_weights.npz\"):\n",
    "        np.savez(file_name, w=self.w)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_name=\"sarsa_weights.npz\"):\n",
    "        data = np.load(file_name)\n",
    "        n_featues = 6\n",
    "        ag = Sarsa(n_actions=data['w'].shape[0] - n_featues)\n",
    "        ag.w = data['w']\n",
    "\n",
    "        print(\"Loaded SARSA agent with rules characteristics:\")\n",
    "        print(\"w shape:\", ag.w.shape)\n",
    "        print(\"w norm:\", np.linalg.norm(ag.w))\n",
    "        print(\"non-zero weights:\", np.count_nonzero(ag.w))\n",
    "        return ag\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def reset_traces(self):\n",
    "        self.z.fill(0.0)"
   ],
   "id": "23f7d9eb90e736ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.158709Z",
     "start_time": "2025-11-13T22:37:48.154704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_model_trained():\n",
    "    try:\n",
    "        _ = np.load(\"sarsa_weights.npz\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "3ea75ff7bcd035e6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.167744Z",
     "start_time": "2025-11-13T22:37:48.163233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file_exist(file_name):\n",
    "    try:\n",
    "        _ = np.load(file_name)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "342b5ae0c6a0411e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.176784Z",
     "start_time": "2025-11-13T22:37:48.173306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_LIVING = -0.001            # MANUAL: Must be a small negative \"living penalty\".\n",
    "R_INACTION = -0.2603       # IRL: Learned from your data.\n",
    "R_WALL = -2.082           # IRL: Learned from your data.\n",
    "# R_INACTION = -1.082       # IRL: Learned from your data.\n",
    "# R_WALL = -1.082           # IRL: Learned from your data.\n",
    "R_PROXIMITY = -0.0499      # IRL: Learned from your data.\n",
    "R_HUNTING = 0.2            # MANUAL: Override. The model failed to learn this.\n",
    "R_KILL = 15.0              # MANUAL: Override. The model failed to learn this.\n",
    "R_DEATH = -20.0            # MANUAL: Override. The model's value was too small."
   ],
   "id": "19f739f20f96bb61",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.197107Z",
     "start_time": "2025-11-13T22:37:48.183305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epsilon_min = 0.05, epsilon_decay_fraction = 0.999, initial_epsilon = 1.0):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_fraction = epsilon_decay_fraction\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_name_for_class(class_name):\n",
    "        return f\"sarsa-weights-{class_name.lower()}.npz\"\n",
    "\n",
    "    def train_if_needed(self, model, env, class_name, n_episodes=1000):\n",
    "        file_name = Trainer._file_name_for_class(class_name)\n",
    "        print(f'Checking for existing model file: {file_name}')\n",
    "        if not file_exist(file_name):\n",
    "            self.train(model, env, class_name, n_episodes)\n",
    "            return model\n",
    "\n",
    "        return Sarsa.load(file_name)\n",
    "\n",
    "    def train(self, model, env, class_name, n_episodes=1000):\n",
    "        print(f\"Training {class_name} agent...\")\n",
    "        # state_observer = StateObserver(w=21, h=21)\n",
    "        action_counts = np.zeros(env.action_space.n, dtype=np.float32)\n",
    "\n",
    "        max_score_ever = -np.inf\n",
    "        rewards = []\n",
    "        w_changes = []\n",
    "        previous_w = model.w.copy()\n",
    "\n",
    "        model.epsilon = self.initial_epsilon\n",
    "        decay_episodes = int(n_episodes * self.epsilon_decay_fraction)\n",
    "        if decay_episodes > 0:\n",
    "             epsilon_decay_step = (self.initial_epsilon - self.epsilon_min) / decay_episodes\n",
    "        else:\n",
    "             epsilon_decay_step = 0\n",
    "        print(f\"Epsilon will decay from {self.initial_epsilon} to {self.epsilon_min} over {decay_episodes} episodes.\")\n",
    "\n",
    "        log_step = max(1, n_episodes // 100)\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            model.reset_traces()\n",
    "            features = np.array(state, dtype=np.float32)\n",
    "            action, q_values = model.epsilon_greedy(features)\n",
    "            action_counts[action] += 1\n",
    "            phi = model.phi_from_state_action(features, action)\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Ensure next_state is properly flattened\n",
    "                next_features = np.array(next_state, dtype=np.float32)\n",
    "                next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                action_counts[next_action] += 1\n",
    "                next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "                if len(q_values) != model.n_actions:\n",
    "                    raise ValueError(f\"Expected q_values of length {model.n_actions}, got {len(q_values)}\")\n",
    "\n",
    "                q = q_values[action]\n",
    "                if done:\n",
    "                    q_next = 0.0\n",
    "                    continue\n",
    "                else:\n",
    "                    # next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "                    q_next = next_q_values[next_action]\n",
    "\n",
    "                    # Store these for the *next* loop iteration\n",
    "\n",
    "                action_counts[next_action] += 1\n",
    "                # next_phi = model.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "\n",
    "                # Use the original state (2D array) for state observer\n",
    "                shaped_reward = reward + R_LIVING\n",
    "                # if action not in MOVE_ACTIONS:\n",
    "                #     shaped_reward += R_INACTION\n",
    "\n",
    "                if action == 2: # UP\n",
    "                    shaped_reward += next_features[0] * R_WALL\n",
    "                elif action == 5: # DOWN\n",
    "                    shaped_reward += next_features[1] * R_WALL # f_wall_down\n",
    "                elif action == 4: # LEFT\n",
    "                    shaped_reward += next_features[2] * R_WALL # f_wall_left\n",
    "                elif action == 3: # RIGHT\n",
    "                    shaped_reward += next_features[3] * R_WALL\n",
    "\n",
    "                if reward > 0:\n",
    "                     shaped_reward += R_KILL\n",
    "\n",
    "                # 4. Hunting Reward\n",
    "                f_enemy_dir_y, f_enemy_dir_x = features[4], features[5]\n",
    "                if f_enemy_dir_y != 0 or f_enemy_dir_x != 0: # If an enemy exists\n",
    "                    if (action == 2 and f_enemy_dir_y == -1.0) or \\\n",
    "                       (action == 5 and f_enemy_dir_y == 1.0) or \\\n",
    "                       (action == 4 and f_enemy_dir_x == -1.0) or \\\n",
    "                       (action == 3 and f_enemy_dir_x == 1.0):\n",
    "                        shaped_reward += R_HUNTING\n",
    "\n",
    "                if terminated:\n",
    "                    shaped_reward += R_DEATH\n",
    "\n",
    "                delta = shaped_reward + model.gamma * q_next - q\n",
    "                model.z = (model.gamma * model.lmbda * model.z) + phi\n",
    "                model.w += model.alpha * delta * model.z\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                q_values = next_q_values\n",
    "                phi = next_phi\n",
    "                ep_reward += reward\n",
    "\n",
    "            new_epsilon = model.epsilon - epsilon_decay_step\n",
    "            model.epsilon = max(self.epsilon_min, new_epsilon)\n",
    "\n",
    "            w_change = np.mean(np.abs(model.w - previous_w))\n",
    "            w_changes.append(w_change)\n",
    "            previous_w = model.w.copy()\n",
    "\n",
    "            rewards.append(ep_reward)\n",
    "            if ep_reward > max_score_ever:\n",
    "                max_score_ever = ep_reward\n",
    "\n",
    "\n",
    "            if (episode + 1) % log_step == 0:\n",
    "                recent_max = float(np.max(rewards[-log_step:])) if len(rewards) > 0 else float(ep_reward)\n",
    "                print(f\"Episode {episode+1}/{n_episodes}: Max reward for period={recent_max:.2f}, Eps={model.epsilon:.4f}\")\n",
    "\n",
    "        px.line(x=np.arange(1, n_episodes + 1), y=w_changes, labels={'x': 'Episode', 'y': 'Mean |Δw|'},\n",
    "                title='Mean Weight Change over Episodes').show()\n",
    "        print(f'Action distribution during training: {action_counts}')\n",
    "        print(f\"Training completed. Max score ever: {max_score_ever:.2f}\")\n",
    "        model.save(self._file_name_for_class(class_name))\n"
   ],
   "id": "f185cf8af1717cf",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.205710Z",
     "start_time": "2025-11-13T22:37:48.202620Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f1e451f2714ca31e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.219751Z",
     "start_time": "2025-11-13T22:37:48.210717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_NAME = \"Berzerk-Default\"\n",
    "\n",
    "agent = Sarsa(env.action_space.n)\n",
    "\n",
    "epsilon_min = 0.1\n",
    "\n",
    "trainer = Trainer(epsilon_min, 0.8, initial_epsilon=0.5)\n",
    "agent = trainer.train_if_needed(agent, env, class_name=CLASS_NAME, n_episodes=1000)\n",
    "\n",
    "env.close()\n"
   ],
   "id": "e0f035ed87e04e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model file: sarsa-weights-berzerk-default.npz\n",
      "Loaded SARSA agent with rules characteristics:\n",
      "w shape: (24,)\n",
      "w norm: 53.34482\n",
      "non-zero weights: 24\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "11c3ee405fb8fe3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.237816Z",
     "start_time": "2025-11-13T22:37:48.232294Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load(Trainer._file_name_for_class(CLASS_NAME))",
   "id": "1701d95cc6b60226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SARSA agent with rules characteristics:\n",
      "w shape: (24,)\n",
      "w norm: 53.34482\n",
      "non-zero weights: 24\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.873123Z",
     "start_time": "2025-11-13T22:37:48.250863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\", frameskip=4)\n",
    "agent.restrict_exploration()"
   ],
   "id": "f40d266430b82729",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:37:48.884344Z",
     "start_time": "2025-11-13T22:37:48.879621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_weights(w):\n",
    "    avg = np.mean(w)\n",
    "    irq = np.percentile(w, 75) - np.percentile(w, 25)\n",
    "    lower_bound = avg - irq\n",
    "    upper_bound = avg + irq\n",
    "    w_clipped = np.clip(w, lower_bound, upper_bound)\n",
    "    return w_clipped\n"
   ],
   "id": "5b0a81ab504daa23",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T22:37:48.893419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "agent.w = normalize_weights(agent.w)\n",
    "print(f'w={agent.w}')\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    actions_count = np.zeros(test_env.action_space.n, dtype=np.int32)\n",
    "    while not done:\n",
    "        features = extract_intelligent_features(state)\n",
    "        action, _ = agent.epsilon_greedy(features)\n",
    "        actions_count[action] += 1\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    test_env.render()\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    print(f'Action count during round: {actions_count}')\n",
    "    print('---------------------------')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "a9f72e2ca473c891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[8.798747 8.798747 9.097671 8.798747 8.798747 8.798747 8.902695 9.004868\n",
      " 8.798747 8.798747 8.798747 8.798747 8.798747 8.798747 8.925439 8.798747\n",
      " 8.961977 8.820185 8.798747 9.097671 8.798747 8.955205 8.798747 8.798747]\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
