{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.199776Z",
     "start_time": "2025-10-28T21:18:37.195254Z"
    }
   },
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import cv2"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.216769Z",
     "start_time": "2025-10-28T21:18:37.211815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, h=21, w=21):\n",
    "        super().__init__(env)\n",
    "        self.h, self.w = h, w\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (h, w, 3), np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = cv2.resize(obs, (self.w, self.h), interpolation=cv2.INTER_NEAREST)\n",
    "        return obs.astype(np.float32) / 255.0\n"
   ],
   "id": "a84c4cf5e7b520a4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.309243Z",
     "start_time": "2025-10-28T21:18:37.221295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "env = ResizeObservation(env, h=21, w=21)\n",
    "observation, info = env.reset()\n"
   ],
   "id": "7c35ab875bfe2e50",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.320222Z",
     "start_time": "2025-10-28T21:18:37.315655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"action_space:\", env.action_space)\n",
    "print(\"n actions:\", env.action_space.n)"
   ],
   "id": "19860050d8c619b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(18)\n",
      "n actions: 18\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.334137Z",
     "start_time": "2025-10-28T21:18:37.328447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    meanings = env.unwrapped.get_action_meanings()\n",
    "except Exception:\n",
    "    try:\n",
    "        meanings = env.get_action_meanings()\n",
    "    except Exception:\n",
    "        meanings = None\n",
    "\n",
    "if meanings:\n",
    "    print(\"Action index -> meaning:\")\n",
    "    for i, name in enumerate(meanings):\n",
    "        print(f\"{i}: {name}\")\n",
    "else:\n",
    "    print(\"No action meanings available from the env. Use index numbers (0..n-1).\")\n"
   ],
   "id": "17d4dbe981372894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action index -> meaning:\n",
      "0: NOOP\n",
      "1: FIRE\n",
      "2: UP\n",
      "3: RIGHT\n",
      "4: LEFT\n",
      "5: DOWN\n",
      "6: UPRIGHT\n",
      "7: UPLEFT\n",
      "8: DOWNRIGHT\n",
      "9: DOWNLEFT\n",
      "10: UPFIRE\n",
      "11: RIGHTFIRE\n",
      "12: LEFTFIRE\n",
      "13: DOWNFIRE\n",
      "14: UPRIGHTFIRE\n",
      "15: UPLEFTFIRE\n",
      "16: DOWNRIGHTFIRE\n",
      "17: DOWNLEFTFIRE\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.345174Z",
     "start_time": "2025-10-28T21:18:37.341656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_frame(frame):\n",
    "    # clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    # plt.show()"
   ],
   "id": "1a691094b8caf081",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.355202Z",
     "start_time": "2025-10-28T21:18:37.351202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_image(frame):\n",
    "    img = Image.fromarray(frame)\n",
    "    # img = img.convert('L')  # Convert to grayscale\n",
    "    img = np.array(img)\n",
    "    img = cv2.resize(img, (21, 21))\n",
    "    img = img / 255.0\n",
    "    # img = img.flatten()\n",
    "    return np.array(img)"
   ],
   "id": "9bf4e15a9d395f52",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.363474Z",
     "start_time": "2025-10-28T21:18:37.360233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "id": "2473be146375f018",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:18:37.375257Z",
     "start_time": "2025-10-28T21:18:37.368480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-4\n",
    "    gamma = 0.99\n",
    "    epsilon = 0.1\n",
    "    feature_h, feature_w = 21, 21\n",
    "    use_traces = False\n",
    "    lmbda = 0.9\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        feature_dim = self.feature_h * self.feature_w * 3 + n_actions\n",
    "        self.w = np.zeros(feature_dim, dtype=np.float32)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def _extract_rgb_features(self, frame):\n",
    "        f = cv2.resize(frame, (self.feature_h, self.feature_w), interpolation=cv2.INTER_AREA)\n",
    "        f = f.astype(np.float32) / 255.0\n",
    "        return f.flatten()\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        # features = self._extract_rgb_features(features)\n",
    "        a_onehot = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        a_onehot[action] = 1.0\n",
    "        return np.concatenate([features, a_onehot])\n",
    "\n",
    "    def q_value(self, phi):\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        features_tiled = np.tile(state_features, (self.n_actions, 1))  # shape (n_actions, state_dim)\n",
    "        eye_actions = np.eye(self.n_actions, dtype=np.float32)\n",
    "        phis = np.concatenate([features_tiled, eye_actions], axis=1)   # shape (n_actions, total_dim)\n",
    "        return np.dot(phis, self.w)\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "        return np.argmax(q_vals)\n",
    "\n",
    "    def save(self):\n",
    "        np.savez(\"sarsa_weights.npz\", w=self.w)\n",
    "\n",
    "    @staticmethod\n",
    "    def load():\n",
    "        data = np.load(\"sarsa_weights.npz\")\n",
    "        agent = Sarsa(n_actions=data['w'].shape[0] - 21*21*3)\n",
    "        agent.w = data['w']\n",
    "        return agent\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0"
   ],
   "id": "8acb4d0de5744958",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:19:05.868588Z",
     "start_time": "2025-10-28T21:18:37.380777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = Sarsa(env.action_space.n)\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    features = np.array(state).flatten()\n",
    "    action = agent.epsilon_greedy(features)\n",
    "    phi = agent.phi_from_state_action(features, action)\n",
    "\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    q_next = None\n",
    "    phi_next = None\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_features = np.array(next_state).flatten()\n",
    "        next_action = agent.epsilon_greedy(next_features)\n",
    "        next_phi = agent.phi_from_state_action(next_features, next_action)\n",
    "\n",
    "        q = np.dot(agent.w, phi)\n",
    "        q_next = np.dot(agent.w, next_phi)\n",
    "        delta = reward + agent.gamma * q_next - q\n",
    "\n",
    "        agent.w += agent.alpha * delta * phi\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        phi = next_phi\n",
    "        ep_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {ep_reward}\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "383bd49de17cecc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 400.0\n",
      "Episode 2: Total Reward: 400.0\n",
      "Episode 3: Total Reward: 100.0\n",
      "Episode 4: Total Reward: 150.0\n",
      "Episode 5: Total Reward: 100.0\n",
      "Episode 6: Total Reward: 550.0\n",
      "Episode 7: Total Reward: 450.0\n",
      "Episode 8: Total Reward: 50.0\n",
      "Episode 9: Total Reward: 450.0\n",
      "Episode 10: Total Reward: 470.0\n",
      "Episode 11: Total Reward: 200.0\n",
      "Episode 12: Total Reward: 0.0\n",
      "Episode 13: Total Reward: 250.0\n",
      "Episode 14: Total Reward: 100.0\n",
      "Episode 15: Total Reward: 50.0\n",
      "Episode 16: Total Reward: 150.0\n",
      "Episode 17: Total Reward: 100.0\n",
      "Episode 18: Total Reward: 640.0\n",
      "Episode 19: Total Reward: 150.0\n",
      "Episode 20: Total Reward: 100.0\n",
      "Episode 21: Total Reward: 200.0\n",
      "Episode 22: Total Reward: 250.0\n",
      "Episode 23: Total Reward: 400.0\n",
      "Episode 24: Total Reward: 100.0\n",
      "Episode 25: Total Reward: 350.0\n",
      "Episode 26: Total Reward: 200.0\n",
      "Episode 27: Total Reward: 560.0\n",
      "Episode 28: Total Reward: 500.0\n",
      "Episode 29: Total Reward: 200.0\n",
      "Episode 30: Total Reward: 640.0\n",
      "Episode 31: Total Reward: 150.0\n",
      "Episode 32: Total Reward: 100.0\n",
      "Episode 33: Total Reward: 200.0\n",
      "Episode 34: Total Reward: 330.0\n",
      "Episode 35: Total Reward: 50.0\n",
      "Episode 36: Total Reward: 200.0\n",
      "Episode 37: Total Reward: 400.0\n",
      "Episode 38: Total Reward: 150.0\n",
      "Episode 39: Total Reward: 300.0\n",
      "Episode 40: Total Reward: 200.0\n",
      "Episode 41: Total Reward: 0.0\n",
      "Episode 42: Total Reward: 300.0\n",
      "Episode 43: Total Reward: 350.0\n",
      "Episode 44: Total Reward: 250.0\n",
      "Episode 45: Total Reward: 350.0\n",
      "Episode 46: Total Reward: 150.0\n",
      "Episode 47: Total Reward: 300.0\n",
      "Episode 48: Total Reward: 150.0\n",
      "Episode 49: Total Reward: 0.0\n",
      "Episode 50: Total Reward: 400.0\n",
      "Episode 51: Total Reward: 300.0\n",
      "Episode 52: Total Reward: 350.0\n",
      "Episode 53: Total Reward: 50.0\n",
      "Episode 54: Total Reward: 300.0\n",
      "Episode 55: Total Reward: 200.0\n",
      "Episode 56: Total Reward: 100.0\n",
      "Episode 57: Total Reward: 100.0\n",
      "Episode 58: Total Reward: 250.0\n",
      "Episode 59: Total Reward: 150.0\n",
      "Episode 60: Total Reward: 500.0\n",
      "Episode 61: Total Reward: 50.0\n",
      "Episode 62: Total Reward: 50.0\n",
      "Episode 63: Total Reward: 50.0\n",
      "Episode 64: Total Reward: 50.0\n",
      "Episode 65: Total Reward: 250.0\n",
      "Episode 66: Total Reward: 490.0\n",
      "Episode 67: Total Reward: 400.0\n",
      "Episode 68: Total Reward: 100.0\n",
      "Episode 69: Total Reward: 300.0\n",
      "Episode 70: Total Reward: 100.0\n",
      "Episode 71: Total Reward: 50.0\n",
      "Episode 72: Total Reward: 50.0\n",
      "Episode 73: Total Reward: 200.0\n",
      "Episode 74: Total Reward: 350.0\n",
      "Episode 75: Total Reward: 500.0\n",
      "Episode 76: Total Reward: 200.0\n",
      "Episode 77: Total Reward: 250.0\n",
      "Episode 78: Total Reward: 400.0\n",
      "Episode 79: Total Reward: 250.0\n",
      "Episode 80: Total Reward: 550.0\n",
      "Episode 81: Total Reward: 50.0\n",
      "Episode 82: Total Reward: 400.0\n",
      "Episode 83: Total Reward: 250.0\n",
      "Episode 84: Total Reward: 500.0\n",
      "Episode 85: Total Reward: 150.0\n",
      "Episode 86: Total Reward: 450.0\n",
      "Episode 87: Total Reward: 300.0\n",
      "Episode 88: Total Reward: 100.0\n",
      "Episode 89: Total Reward: 350.0\n",
      "Episode 90: Total Reward: 100.0\n",
      "Episode 91: Total Reward: 150.0\n",
      "Episode 92: Total Reward: 250.0\n",
      "Episode 93: Total Reward: 340.0\n",
      "Episode 94: Total Reward: 350.0\n",
      "Episode 95: Total Reward: 300.0\n",
      "Episode 96: Total Reward: 200.0\n",
      "Episode 97: Total Reward: 300.0\n",
      "Episode 98: Total Reward: 200.0\n",
      "Episode 99: Total Reward: 300.0\n",
      "Episode 100: Total Reward: 450.0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:19:05.893927Z",
     "start_time": "2025-10-28T21:19:05.890439Z"
    }
   },
   "cell_type": "code",
   "source": "agent.save()",
   "id": "a72585b0456ddaa5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:19:05.902883Z",
     "start_time": "2025-10-28T21:19:05.898444Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load()",
   "id": "1fb8f39f979914c1",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:19:06.553004Z",
     "start_time": "2025-10-28T21:19:05.908397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\")\n",
    "agent.restrict_exploration()"
   ],
   "id": "bde1bebc2b701858",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:19:06.613985Z",
     "start_time": "2025-10-28T21:19:06.558522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 10\n",
    "total_rewards = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.epsilon_greedy(state)  # now deterministic\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "a6d244c1a09c42ab",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m ep_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m---> 10\u001B[0m     action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mepsilon_greedy(state)  \u001B[38;5;66;03m# now deterministic\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     next_state, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     12\u001B[0m     done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "Cell \u001B[1;32mIn[24], line 38\u001B[0m, in \u001B[0;36mSarsa.epsilon_greedy\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_actions)\n\u001B[1;32m---> 38\u001B[0m q_vals \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_q_values_all_actions(features)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax(q_vals)\n",
      "Cell \u001B[1;32mIn[24], line 31\u001B[0m, in \u001B[0;36mSarsa._q_values_all_actions\u001B[1;34m(self, state_features)\u001B[0m\n\u001B[0;32m     29\u001B[0m features_tiled \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtile(state_features, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_actions, \u001B[38;5;241m1\u001B[39m))  \u001B[38;5;66;03m# shape (n_actions, state_dim)\u001B[39;00m\n\u001B[0;32m     30\u001B[0m eye_actions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_actions, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 31\u001B[0m phis \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([features_tiled, eye_actions], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)   \u001B[38;5;66;03m# shape (n_actions, total_dim)\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(phis, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw)\n",
      "\u001B[1;31mValueError\u001B[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
