{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.017821Z",
     "start_time": "2025-11-15T22:17:09.760663Z"
    }
   },
   "source": [
    "from ale_py import ALEInterface\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "import numba\n",
    "import plotly.express as px\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.030419Z",
     "start_time": "2025-11-15T22:17:11.025964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIRE_ACTIONS = [1, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "MOVE_ACTIONS = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "np.random.seed(42)"
   ],
   "id": "c8310881c0d32161",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.044160Z",
     "start_time": "2025-11-15T22:17:11.040019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "UP_DIRECTION = 0\n",
    "RIGHT_DIRECTION = 1\n",
    "LEFT_DIRECTION = 2\n",
    "DOWN_DIRECTION = 3\n",
    "UPRIGHT_DIRECTION = 4\n",
    "UPLEFT_DIRECTION = 5\n",
    "DOWNRIGHT_DIRECTION = 6\n",
    "DOWNLEFT_DIRECTION = 7"
   ],
   "id": "f1db9cfae656a91",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.060506Z",
     "start_time": "2025-11-15T22:17:11.055373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMPTY_COLOR = np.array([0, 0, 0], dtype=np.uint8)\n",
    "WALL_COLOR = np.array([84, 92, 214], dtype=np.uint8)\n",
    "ENEMY_COLOR = np.array([210, 210, 64], dtype=np.uint8)\n",
    "PLAYER_COLOR = np.array([240, 170, 103], dtype=np.uint8)\n",
    "\n",
    "EMPTY_INDEX = 0\n",
    "WALL_INDEX = 1\n",
    "ENEMY_INDEX = 2\n",
    "PLAYER_INDEX = 3\n",
    "\n",
    "PORTAL_INDEX = 4  # add manualy\n",
    "PORTAL_COLOR = np.array([74, 255, 56], dtype=np.uint8)\n",
    "\n",
    "AVG_PIXELS_IN_ENEMY = 74\n",
    "MAX_ENEMIES = 8"
   ],
   "id": "3b52870bd5f84e85",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.094321Z",
     "start_time": "2025-11-15T22:17:11.069513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@numba.njit\n",
    "def rgb_to_index(frame):\n",
    "    state = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
    "    enemy_pixel_count = 0\n",
    "    for i in range(frame.shape[0]):\n",
    "        for j in range(frame.shape[1]):\n",
    "            pixel = frame[i, j]\n",
    "            if pixel[0] == WALL_COLOR[0]:\n",
    "                state[i, j] = WALL_INDEX\n",
    "            elif pixel[0] == ENEMY_COLOR[0]:\n",
    "                state[i, j] = ENEMY_INDEX\n",
    "                enemy_pixel_count += 1\n",
    "            elif pixel[0] == PLAYER_COLOR[0]:\n",
    "                state[i, j] = PLAYER_INDEX\n",
    "            elif pixel[0] == PORTAL_COLOR[0]:\n",
    "                state[i, j] = PORTAL_INDEX\n",
    "            else:\n",
    "                state[i, j] = EMPTY_INDEX\n",
    "    return state, enemy_pixel_count\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def cut_empty_layers(state):\n",
    "    skip_layers = 0\n",
    "    while True:\n",
    "        if state[skip_layers][skip_layers] == 0:\n",
    "            skip_layers += 1\n",
    "        else:\n",
    "            break\n",
    "    state = state[skip_layers:-skip_layers, skip_layers:-skip_layers]\n",
    "\n",
    "    skip_layers_from_bottom = 0\n",
    "    while True:\n",
    "        if state[-(skip_layers_from_bottom + 1)][-(skip_layers_from_bottom + 1)] == 0:\n",
    "            skip_layers_from_bottom += 1\n",
    "        else:\n",
    "            break\n",
    "    state = state[:-skip_layers_from_bottom, :]\n",
    "    return state\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def fill_part(matrix, a, b, c, d, fill_value):\n",
    "    rows, cols = matrix.shape\n",
    "    min_i = max(0, min(a[0], b[0], c[0], d[0]))\n",
    "    max_i = min(rows - 1, max(a[0], b[0], c[0], d[0]))\n",
    "    min_j = max(0, min(a[1], b[1], c[1], d[1]))\n",
    "    max_j = min(cols - 1, max(a[1], b[1], c[1], d[1]))\n",
    "\n",
    "    for ii in range(min_i, max_i + 1):\n",
    "        for jj in range(min_j, max_j + 1):\n",
    "            if matrix[ii, jj] == EMPTY_INDEX:\n",
    "                matrix[ii, jj] = fill_value\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def fill_holes(state):\n",
    "    PORTAL_MIN_LENGHT = 3\n",
    "    PORTAL_WIDTH = 0\n",
    "    WALL_WIDTH = 3\n",
    "\n",
    "    if state[0][0] != WALL_INDEX:\n",
    "        return\n",
    "\n",
    "    P = 2 * (state.shape[0] + state.shape[1])\n",
    "\n",
    "    i, j = 0, 0\n",
    "    height, width = state.shape\n",
    "    last_wall_pixel = (0, 0)\n",
    "    iters = 0\n",
    "    while True:\n",
    "        if iters > P + 1:\n",
    "            raise Warning(\"No walls found on border during hole filling\")\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if state[i][j] == WALL_INDEX:\n",
    "            distance_to_last_wall = max(abs(i - last_wall_pixel[0]), abs(j - last_wall_pixel[1]))\n",
    "            if distance_to_last_wall == 1:\n",
    "                pass\n",
    "            else:\n",
    "                material_to_fill = WALL_INDEX if distance_to_last_wall < PORTAL_MIN_LENGHT else PORTAL_INDEX\n",
    "                width_to_fill = WALL_WIDTH if material_to_fill == WALL_INDEX else PORTAL_WIDTH\n",
    "                A, B, C, D = None, None, None, None\n",
    "                if j == 0:\n",
    "                    A = (last_wall_pixel[0] + 1, last_wall_pixel[1])\n",
    "                    B = (last_wall_pixel[0] + 1, last_wall_pixel[1] + width_to_fill)\n",
    "                    C = (i, j + width_to_fill)\n",
    "                    D = (i, j)\n",
    "                elif i == height - 1:\n",
    "                    A = (last_wall_pixel[0], last_wall_pixel[1] + 1)\n",
    "                    B = (last_wall_pixel[0] - width_to_fill, last_wall_pixel[1] + 1)\n",
    "                    C = (i - width_to_fill, j)\n",
    "                    D = (i, j)\n",
    "                elif j == width - 1:\n",
    "                    A = (last_wall_pixel[0] - 1, last_wall_pixel[1])\n",
    "                    B = (last_wall_pixel[0] - 1, last_wall_pixel[1] - width_to_fill)\n",
    "                    C = (i, j - width_to_fill)\n",
    "                    D = (i, j)\n",
    "                elif i == 0:\n",
    "                    A = (last_wall_pixel[0], last_wall_pixel[1] - 1)\n",
    "                    B = (last_wall_pixel[0] - width_to_fill, last_wall_pixel[1] - 1)\n",
    "                    C = (i + width_to_fill, j)\n",
    "                    D = (i, j)\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected wall pixel not on border\")\n",
    "\n",
    "                fill_part(state, A, B, C, D, material_to_fill)\n",
    "\n",
    "            last_wall_pixel = (i, j)\n",
    "\n",
    "        if j == 0 and i < height - 1:\n",
    "            i += 1\n",
    "        elif i == height - 1 and j < width - 1:\n",
    "            j += 1\n",
    "        elif j == width - 1 and i > 0:\n",
    "            i -= 1\n",
    "        elif i == 0 and j > 0:\n",
    "            j -= 1\n",
    "\n",
    "        if i == 0 and j == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def find_player_box(state):\n",
    "    player_positions = np.argwhere(state == PLAYER_INDEX)\n",
    "\n",
    "    if player_positions.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    i_pos = player_positions[:, 0]\n",
    "    j_pos = player_positions[:, 1]\n",
    "    box = (np.min(i_pos), np.min(j_pos), np.max(i_pos), np.max(j_pos))\n",
    "    return box\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def basic_observation(state, box):\n",
    "    if box is None:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    left_search_point = ((box[0] + box[2]) // 2, box[1])\n",
    "    right_search_point = ((box[0] + box[2]) // 2, box[3])\n",
    "    up_search_point = (box[0], (box[1] + box[3]) // 2)\n",
    "    down_search_point = (box[2], (box[1] + box[3]) // 2)\n",
    "\n",
    "    l_wall = (left_search_point[0], 0)\n",
    "    r_wall = (right_search_point[0], state.shape[1] - 1)\n",
    "    u_wall = (0, up_search_point[1])\n",
    "    d_wall = (state.shape[0] - 1, down_search_point[1])\n",
    "\n",
    "    reach_l, reach_r, reach_u, reach_d = False, False, False, False\n",
    "\n",
    "    closest_enemy = None\n",
    "    closest_portal = None\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i > max(state.shape):\n",
    "            raise ValueError(\"No walls found in any direction\")\n",
    "\n",
    "        if not reach_l:\n",
    "            cord = (left_search_point[0], left_search_point[1] - i)\n",
    "            material = state[cord]\n",
    "            if material == WALL_INDEX or cord[1] == 0:\n",
    "                reach_l = True\n",
    "                l_wall = cord\n",
    "            if material == PORTAL_INDEX:\n",
    "                reach_l = True\n",
    "                if closest_portal is None:\n",
    "                    closest_portal = cord\n",
    "            elif material == ENEMY_INDEX:\n",
    "                if closest_enemy is None:\n",
    "                    closest_enemy = cord\n",
    "        if not reach_r:\n",
    "            cord = (right_search_point[0], right_search_point[1] + i)\n",
    "            material = state[cord]\n",
    "            if material == WALL_INDEX or cord[1] == state.shape[1] - 1:\n",
    "                reach_r = True\n",
    "                r_wall = cord\n",
    "            elif material == PORTAL_INDEX:\n",
    "                reach_r = True\n",
    "                if closest_portal is None:\n",
    "                    closest_portal = cord\n",
    "            elif material == ENEMY_INDEX:\n",
    "                if closest_enemy is None:\n",
    "                    closest_enemy = cord\n",
    "        if not reach_u:\n",
    "            cord = (up_search_point[0] - i, up_search_point[1])\n",
    "            material = state[cord]\n",
    "            if material == WALL_INDEX or cord[0] == 0:\n",
    "                reach_u = True\n",
    "                u_wall = cord\n",
    "            elif material == PORTAL_INDEX:\n",
    "                reach_u = True\n",
    "                if closest_portal is None:\n",
    "                    closest_portal = cord\n",
    "            elif material == ENEMY_INDEX:\n",
    "                if closest_enemy is None:\n",
    "                    closest_enemy = cord\n",
    "        if not reach_d:\n",
    "            cord = (down_search_point[0] + i, down_search_point[1])\n",
    "            material = state[cord]\n",
    "            if material == WALL_INDEX or cord[0] == state.shape[0] - 1:\n",
    "                reach_d = True\n",
    "                d_wall = cord\n",
    "            if material == PORTAL_INDEX:\n",
    "                reach_d = True\n",
    "                if closest_portal is None:\n",
    "                    closest_portal = cord\n",
    "            elif material == ENEMY_INDEX:\n",
    "                if closest_enemy is None:\n",
    "                    closest_enemy = cord\n",
    "        if reach_l and reach_r and reach_u and reach_d:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return l_wall, r_wall, u_wall, d_wall, closest_enemy, closest_portal\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def has_enemy(state):\n",
    "    return np.any(state == ENEMY_INDEX)\n"
   ],
   "id": "5aa432d125afccc7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.106369Z",
     "start_time": "2025-11-15T22:17:11.101326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@numba.njit\n",
    "def cut_empty_layers_in_frame(frame):\n",
    "    skip_layers = 0\n",
    "    while True:\n",
    "        if np.array_equal(frame[skip_layers][skip_layers], EMPTY_COLOR):\n",
    "            skip_layers += 1\n",
    "        else:\n",
    "            break\n",
    "    frame = frame[skip_layers:-skip_layers, skip_layers:-skip_layers]\n",
    "\n",
    "    skip_layers_from_bottom = 0\n",
    "    while True:\n",
    "        if np.array_equal(frame[-(skip_layers_from_bottom + 1)][-(skip_layers_from_bottom + 1)], EMPTY_COLOR):\n",
    "            skip_layers_from_bottom += 1\n",
    "        else:\n",
    "            break\n",
    "    frame = frame[:-skip_layers_from_bottom, :]\n",
    "    return frame\n",
    "\n",
    "@numba.njit\n",
    "def from_frame_to_feature_state(frame):\n",
    "    frame = cut_empty_layers_in_frame(frame)\n"
   ],
   "id": "45f5c0abc04aa454",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.125771Z",
     "start_time": "2025-11-15T22:17:11.114380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VECTOR_STATE_SIZE = 9\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, frame):\n",
    "        frame = cut_empty_layers_in_frame(frame)\n",
    "        state, enemy_pixels = rgb_to_index(frame)\n",
    "        self.is_empty = len(state) == 0 or len(state[0]) == 0\n",
    "        if not self.is_empty:\n",
    "            fill_holes(state)\n",
    "            self.state = state\n",
    "            self.state_h = state.shape[0]\n",
    "            self.state_w = state.shape[1]\n",
    "            self.player_box = find_player_box(state)\n",
    "            obs = basic_observation(state, self.player_box) if not self.player_box is None else None\n",
    "\n",
    "            self.left_wall = obs[0] if obs is not None and obs[0] is not None else (0, 0)\n",
    "            self.right_wall = obs[1] if obs is not None and obs[1] is not None else (0, 0)\n",
    "            self.up_wall = obs[2] if obs is not None and obs[2] is not None else (0, 0)\n",
    "            self.down_wall = obs[3] if obs is not None and obs[3] is not None else (0, 0)\n",
    "            self.closest_enemy = obs[4] if obs is not None and obs[4] is not None else None\n",
    "            self.closest_portal = obs[5] if obs is not None and obs[5] is not None else None\n",
    "            self.enemies = np.round(float(enemy_pixels) / AVG_PIXELS_IN_ENEMY).astype(np.int32)\n",
    "\n",
    "    def has_enemy(self):\n",
    "        return has_enemy(self.state)\n",
    "\n",
    "    def get_direction_on_closest_wall(self):\n",
    "        center_of_player = self.center_of_player()\n",
    "        up_dist = center_of_player[0] - self.up_wall[0]\n",
    "        down_dist = self.down_wall[0] - center_of_player[0]\n",
    "        left_dist = center_of_player[1] - self.left_wall[1]\n",
    "        right_dist = self.right_wall[1] - center_of_player[1]\n",
    "\n",
    "        dists = [up_dist, right_dist, left_dist, down_dist]\n",
    "        min_dist = min(dists)\n",
    "        return dists.index(min_dist)\n",
    "\n",
    "    def as_vector(self):\n",
    "        player_center = self.center_of_player()\n",
    "        px_norm = player_center[0] / self.state_h\n",
    "        py_norm = player_center[1] / self.state_w\n",
    "\n",
    "        up_wall_dist = player_center[0] - self.up_wall[0]\n",
    "        down_wall_dist = self.down_wall[0] - player_center[0]\n",
    "        left_wall_dist = player_center[1] - self.left_wall[1]\n",
    "        right_wall_dist = self.right_wall[1] - player_center[1]\n",
    "\n",
    "        dist_up_norm = up_wall_dist / self.state_h\n",
    "        dist_down_norm = down_wall_dist / self.state_h\n",
    "        dist_left_norm = left_wall_dist / self.state_w\n",
    "        dist_right_norm = right_wall_dist / self.state_w\n",
    "\n",
    "        enemy_x_norm = self.closest_enemy[0] / self.state_h if self.closest_enemy is not None else 0\n",
    "        enemy_y_norm = self.closest_enemy[1] / self.state_w if self.closest_enemy is not None else 0\n",
    "        enemy_visible = 1.0 if self.closest_enemy is not None else 0.0\n",
    "\n",
    "        return np.array([px_norm, py_norm,\n",
    "                         dist_up_norm, dist_down_norm,\n",
    "                         dist_left_norm, dist_right_norm,\n",
    "                         enemy_x_norm, enemy_y_norm,\n",
    "                         enemy_visible], dtype=np.float32)\n",
    "\n",
    "    def center_of_player(self):\n",
    "        if self.player_box is None:\n",
    "            return 0, 0\n",
    "        i_center = (self.player_box[0] + self.player_box[2]) // 2\n",
    "        j_center = (self.player_box[1] + self.player_box[3]) // 2\n",
    "        return i_center, j_center\n",
    "\n",
    "    def distance_from_player(self, cord, failure_val=-1):\n",
    "        if self.is_empty or self.player_box is None:\n",
    "            return failure_val\n",
    "\n",
    "        player_x, player_y = self.center_of_player()\n",
    "        return np.sqrt((player_x - cord[0]) ** 2 + (player_y - cord[1]) ** 2)\n",
    "\n",
    "    def distance_to_closest_border(self):\n",
    "        player_x, player_y = self.center_of_player()\n",
    "        up_dist = player_x - self.up_wall[0]\n",
    "        down_dist = self.down_wall[0] - player_x\n",
    "        left_dist = player_y - self.left_wall[1]\n",
    "        right_dist = self.right_wall[1] - player_y\n",
    "        return min(up_dist, down_dist, left_dist, right_dist)\n",
    "\n",
    "    def direction_to_player(self, cord):\n",
    "        i, j = cord\n",
    "        left_up_corner = (self.player_box[0], self.player_box[1])\n",
    "        left_down_corner = (self.player_box[2], self.player_box[1])\n",
    "        right_up_corner = (self.player_box[0], self.player_box[3])\n",
    "        right_down_corner = (self.player_box[2], self.player_box[3])\n",
    "\n",
    "        if i < left_up_corner[0]:\n",
    "            if j < left_up_corner[1]:\n",
    "                return UPLEFT_DIRECTION\n",
    "            elif j > right_up_corner[1]:\n",
    "                return UPRIGHT_DIRECTION\n",
    "            else:\n",
    "                return UP_DIRECTION\n",
    "        elif i > left_down_corner[0]:\n",
    "            if j < left_down_corner[1]:\n",
    "                return DOWNLEFT_DIRECTION\n",
    "            elif j > right_down_corner[1]:\n",
    "                return DOWNRIGHT_DIRECTION\n",
    "            else:\n",
    "                return DOWN_DIRECTION\n",
    "        else:\n",
    "            if j < left_up_corner[1]:\n",
    "                return LEFT_DIRECTION\n",
    "            elif j > right_up_corner[1]:\n",
    "                return RIGHT_DIRECTION\n",
    "            else:\n",
    "                return -1\n"
   ],
   "id": "64756db9b5892c5c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.141903Z",
     "start_time": "2025-11-15T22:17:11.133808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    alpha = 1e-5\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    lmbda = 0.9\n",
    "    weight_decay = 1e-5\n",
    "    z_clip = 10.0\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.state_dim = VECTOR_STATE_SIZE\n",
    "        self.n_actions = n_actions\n",
    "        self.w = np.zeros((self.n_actions, self.state_dim), dtype=np.float32)\n",
    "        self.b = np.zeros(self.n_actions, dtype=np.float32)\n",
    "        self.z_w = np.zeros_like(self.w, dtype=np.float32)\n",
    "        self.z_b = np.zeros((self.n_actions,), dtype=np.float32)\n",
    "\n",
    "    def phi_from_state_action(self, features, action):\n",
    "        phi_w = np.zeros_like(self.w, dtype=np.float32)\n",
    "        phi_w[action] = features.astype(np.float32)\n",
    "        phi_b = np.zeros_like(self.b, dtype=np.float32)\n",
    "        phi_b[action] = 1.0\n",
    "        return phi_w, phi_b\n",
    "\n",
    "    def _q_values_all_actions(self, state_features):\n",
    "        return self.w.dot(state_features.astype(np.float32)) + self.b\n",
    "\n",
    "    def epsilon_greedy(self, features):\n",
    "        eps = float(getattr(self, \"epsilon\", 0.0))\n",
    "        eps = max(0.0, min(1.0, eps))\n",
    "\n",
    "        q_vals = self._q_values_all_actions(features)\n",
    "\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "        return action, q_vals\n",
    "\n",
    "    def save(self, file_name=\"sarsa_weights.npz\"):\n",
    "        np.savez(file_name, w=self.w.reshape(-1), b=self.b, n_actions=self.n_actions, state_dim=self.state_dim)\n",
    "\n",
    "    def restrict_exploration(self):\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def reset_traces(self):\n",
    "        self.z_w.fill(0.0)\n",
    "        self.z_b.fill(0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_name=\"sarsa_weights.npz\"):\n",
    "        data = np.load(file_name)\n",
    "        n_actions = int(data['n_actions'])\n",
    "        state_dim = int(data['state_dim'])\n",
    "        agent = Sarsa(n_actions)\n",
    "        agent.state_dim = state_dim\n",
    "        agent.w = data['w'].reshape((n_actions, state_dim)).astype(np.float32)\n",
    "        agent.b = data['b'].astype(np.float32) if 'b' in data else np.zeros(n_actions, dtype=np.float32)\n",
    "        agent.z_w = np.zeros_like(agent.w, dtype=np.float32)\n",
    "        agent.z_b = np.zeros_like(agent.b, dtype=np.float32)\n",
    "        return agent"
   ],
   "id": "63d46dcf00d5b664",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.153Z",
     "start_time": "2025-11-15T22:17:11.148909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file_exist(file_name):\n",
    "    try:\n",
    "        _ = np.load(file_name)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False"
   ],
   "id": "6d913d1000a89dc4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.167896Z",
     "start_time": "2025-11-15T22:17:11.161010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Examinator:\n",
    "    NOT_SHOOT_ENEMY_PENALTY = -0.7  # reduced from -1.5\n",
    "    LIVING_PENALTY = -0.04  # reduced from -0.1\n",
    "    DEATH_PENALTY = -20  # increased from -12\n",
    "    STAY_IN_DANGER_PENALTY = -0.4  # reduced from -0.5\n",
    "    FIRE_ENEMY_BONUS = 2  # reduced from 6.0 (or set to 0.0)\n",
    "    FAR_FROM_WALL_BONUS = 0.1  # reduced from 0.03\n",
    "    MOVE_BONUS = 0.014\n",
    "    BONUS_FOR_NOT_SHOOT_NOWHERE = 0.33\n",
    "    GO_TO_WALL_PENALTY = -0.1\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def examine(self, state, action, model, reward, prev_state):\n",
    "        shaped_reward = reward / 15\n",
    "\n",
    "        shaped_reward += self.LIVING_PENALTY\n",
    "\n",
    "        if state.closest_enemy is not None:\n",
    "            enemy_dir = state.direction_to_player(state.closest_enemy)\n",
    "            required_action = FIRE_ACTIONS[1] + enemy_dir\n",
    "            if action != required_action and reward <= 0:\n",
    "                shaped_reward += Examinator.NOT_SHOOT_ENEMY_PENALTY\n",
    "            if action == 0:\n",
    "                shaped_reward += Examinator.STAY_IN_DANGER_PENALTY\n",
    "            if action == required_action:\n",
    "                shaped_reward += Examinator.FIRE_ENEMY_BONUS\n",
    "        else:\n",
    "            if action not in FIRE_ACTIONS and action != 0:\n",
    "                shaped_reward += Examinator.BONUS_FOR_NOT_SHOOT_NOWHERE\n",
    "\n",
    "        alive = state.player_box is not None\n",
    "        if not alive:\n",
    "            shaped_reward += Examinator.DEATH_PENALTY\n",
    "\n",
    "        min_distance_to_wall = min(\n",
    "            state.center_of_player()[0] - state.up_wall[0],\n",
    "            state.down_wall[0] - state.center_of_player()[0],\n",
    "            state.center_of_player()[1] - state.left_wall[1],\n",
    "            state.right_wall[1] - state.center_of_player()[1]\n",
    "        )\n",
    "        if min_distance_to_wall > 15:\n",
    "            shaped_reward += Examinator.FAR_FROM_WALL_BONUS\n",
    "\n",
    "        if action in MOVE_ACTIONS:\n",
    "            dir_to_closest_wall = state.get_direction_on_closest_wall()\n",
    "            if action - 2 == dir_to_closest_wall:\n",
    "                shaped_reward += Examinator.GO_TO_WALL_PENALTY\n",
    "            else:\n",
    "                shaped_reward += Examinator.MOVE_BONUS\n",
    "\n",
    "        if state.enemies == 0:\n",
    "            shaped_reward += 0.01 #small for clearing level\n",
    "\n",
    "            # rewarding to be near walls when no enemies to find portals\n",
    "            distance_to_closest_wall = state.distance_to_closest_border()\n",
    "            if 15 < distance_to_closest_wall < 30:\n",
    "                shaped_reward += 0.1\n",
    "            elif 30 <= distance_to_closest_wall < 40:\n",
    "                shaped_reward += 0.08\n",
    "\n",
    "            if state.closest_portal is not None:\n",
    "                #bonus for finding portal\n",
    "                if prev_state.closest_portal is None:\n",
    "                    shaped_reward += 5.0\n",
    "                else:\n",
    "                    prev_distance = prev_state.distance_from_player(prev_state.closest_portal)\n",
    "                    curr_distance = state.distance_from_player(state.closest_portal)\n",
    "                    if curr_distance < prev_distance:\n",
    "                        shaped_reward += 0.5\n",
    "\n",
    "            if state.closest_portal is None and prev_state.closest_portal is not None:\n",
    "                #penalty for losing portal\n",
    "                shaped_reward += -5.0\n",
    "\n",
    "        return shaped_reward\n"
   ],
   "id": "e551160f85718ea2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.180776Z",
     "start_time": "2025-11-15T22:17:11.175934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LastActionTracker:\n",
    "    def __init__(self, space_size):\n",
    "        self.space_size = space_size\n",
    "        self.actions = []\n",
    "\n",
    "    def rec(self, action):\n",
    "        self.actions.append(action)\n",
    "        if len(self.actions) > self.space_size:\n",
    "            self.actions.pop(0)\n",
    "\n",
    "    def last_same_count(self):\n",
    "        if not self.actions:\n",
    "            return 0\n",
    "        last_action = self.actions[-1]\n",
    "        count = 0\n",
    "        for action in reversed(self.actions):\n",
    "            if action == last_action:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count"
   ],
   "id": "c5ea0e00870b528b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.207787Z",
     "start_time": "2025-11-15T22:17:11.190783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epsilon_min=0.05, epsilon_decay_fraction=0.999, initial_epsilon=1.0):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_fraction = epsilon_decay_fraction\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_name_for_class(class_name):\n",
    "        return f\"sarsa-weights-{class_name.lower()}.npz\"\n",
    "\n",
    "    def train_if_needed(self, model, env, class_name, n_episodes=1000):\n",
    "        file_name = Trainer._file_name_for_class(class_name)\n",
    "        print(f'Checking for existing model file: {file_name}')\n",
    "        if not file_exist(file_name):\n",
    "            self.train(model, env, class_name, n_episodes)\n",
    "            return model\n",
    "\n",
    "        return Sarsa.load(file_name)\n",
    "\n",
    "    def train(self, model, env, class_name, n_episodes=1000):\n",
    "        print(f\"Training {class_name} agent...\")\n",
    "        action_counts = np.zeros(env.action_space.n, dtype=np.float32)\n",
    "        examinator = Examinator()\n",
    "\n",
    "        rewards = []\n",
    "        w_changes = []\n",
    "        previous_w = model.w.copy()\n",
    "\n",
    "        model.epsilon = self.initial_epsilon\n",
    "        decay_episodes = int(n_episodes * self.epsilon_decay_fraction)\n",
    "        if decay_episodes > 0:\n",
    "            epsilon_decay_step = (self.initial_epsilon - self.epsilon_min) / decay_episodes\n",
    "        else:\n",
    "            epsilon_decay_step = 0\n",
    "        print(f\"Epsilon will decay from {self.initial_epsilon} to {self.epsilon_min} over {decay_episodes} episodes.\")\n",
    "\n",
    "        log_step = max(1, n_episodes // 100)\n",
    "        action_duplicate_tolerance = 8\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            _ = env.reset()\n",
    "\n",
    "            for j in range(0, 6): # skip initial no-op frames\n",
    "                _ = env.step(0)\n",
    "\n",
    "            last_action_tracker = LastActionTracker(space_size=action_duplicate_tolerance)\n",
    "\n",
    "            state = env.render()\n",
    "            featured_state = State(state)\n",
    "            state_vector = featured_state.as_vector()\n",
    "            distance_to_closest_enemy = featured_state.distance_from_player(\n",
    "                featured_state.closest_enemy) if featured_state.closest_enemy is not None else -1\n",
    "            model.reset_traces()\n",
    "\n",
    "            action, q_values = model.epsilon_greedy(state_vector)\n",
    "            action_counts[action] += 1\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_featured_state = State(next_state)\n",
    "\n",
    "                if next_featured_state.is_empty:\n",
    "                    done = True\n",
    "                    shaped_reward = reward + Examinator.DEATH_PENALTY\n",
    "                    q_next = 0.0\n",
    "                    next_action = None\n",
    "                else:\n",
    "                    done = terminated or truncated or next_featured_state.player_box is None\n",
    "                    next_features = next_featured_state.as_vector()\n",
    "                    next_action, next_q_values = model.epsilon_greedy(next_features)\n",
    "\n",
    "                    shaped_reward = examinator.examine(next_featured_state, action, model, reward, featured_state)\n",
    "\n",
    "                    if last_action_tracker.last_same_count() >= action_duplicate_tolerance and action == next_action:\n",
    "                        shaped_reward += -0.2\n",
    "\n",
    "                    new_distance_to_closest_enemy = next_featured_state.distance_from_player(\n",
    "                        next_featured_state.closest_enemy) if next_featured_state.closest_enemy is not None else -1\n",
    "\n",
    "                    if new_distance_to_closest_enemy != -1 and distance_to_closest_enemy != -1 and distance_to_closest_enemy - new_distance_to_closest_enemy > 0 and new_distance_to_closest_enemy < 20:\n",
    "                        shaped_reward += -0.02\n",
    "                    distance_to_closest_enemy = new_distance_to_closest_enemy\n",
    "\n",
    "                    q_next = 0.0 if done else next_q_values[next_action]\n",
    "\n",
    "                q = q_values[action]\n",
    "                delta = shaped_reward + model.gamma * q_next - q\n",
    "\n",
    "                phi_w, phi_b = model.phi_from_state_action(state_vector, action)\n",
    "\n",
    "                model.z_w = (model.gamma * model.lmbda * model.z_w) + phi_w\n",
    "                model.z_b = (model.gamma * model.lmbda * model.z_b) + phi_b\n",
    "\n",
    "                model.w += model.alpha * delta * model.z_w\n",
    "                model.b += model.alpha * delta * model.z_b\n",
    "\n",
    "                model.w *= (1.0 - model.weight_decay)\n",
    "                model.b *= (1.0 - model.weight_decay)\n",
    "\n",
    "                model.z_w = np.clip(model.z_w, -model.z_clip, model.z_clip)\n",
    "                model.z_b = np.clip(model.z_b, -model.z_clip, model.z_clip)\n",
    "\n",
    "                if not done:\n",
    "                    featured_state = next_featured_state\n",
    "                    state_vector = next_features\n",
    "                    action = next_action\n",
    "                    q_values = next_q_values\n",
    "                    action_counts[action] += 1\n",
    "                    last_action_tracker.rec(action)\n",
    "\n",
    "                ep_reward += reward\n",
    "\n",
    "            if episode > 0 and episode % 5 == 0:\n",
    "                action_freq = action_counts / max(1, action_counts.sum())\n",
    "                action_entropy = -np.sum(action_freq * np.log(action_freq + 1e-10))\n",
    "                target_entropy = np.log(env.action_space.n) * 0.65\n",
    "\n",
    "                if action_entropy < target_entropy and episode < decay_episodes:\n",
    "                    most_used = np.argmax(action_counts)\n",
    "                    second_most = np.argsort(action_counts)[-2]\n",
    "\n",
    "                    # Penalize top 2 most-used actions\n",
    "                    model.b[most_used] *= 0.85\n",
    "                    model.b[second_most] *= 0.92\n",
    "\n",
    "                    # Boost least-used actions\n",
    "                    least_used_indices = np.where(action_freq < 0.02)[0]\n",
    "                    for idx in least_used_indices:\n",
    "                        model.b[idx] *= 1.05\n",
    "\n",
    "                    if episode % 50 == 0:\n",
    "                        print(\n",
    "                            f\"  [Episode {episode}] Entropy={action_entropy:.2f}, most_used={most_used} ({action_freq[most_used] * 100:.1f}%), bias_penalty applied\")\n",
    "\n",
    "            model.epsilon = max(self.epsilon_min, model.epsilon - epsilon_decay_step)\n",
    "\n",
    "            w_change = np.mean(np.abs(model.w - previous_w))\n",
    "            w_changes.append(w_change)\n",
    "            previous_w = model.w.copy()\n",
    "            rewards.append(ep_reward)\n",
    "\n",
    "            if (episode + 1) % log_step == 0:\n",
    "                recent_max = float(np.max(rewards[-log_step:])) if len(rewards) > 0 else float(ep_reward)\n",
    "                print(\n",
    "                    f\"Episode {episode + 1}/{n_episodes}: Max reward for period={recent_max:.2f}, Eps={model.epsilon:.4f}\")\n",
    "\n",
    "        px.line(x=np.arange(1, n_episodes + 1), y=w_changes, labels={'x': 'Episode', 'y': 'Mean |Δw|'},\n",
    "                title='Mean Weight Change over Episodes').show()\n",
    "        px.line(x=np.arange(1, n_episodes + 1), y=rewards, labels={'x': 'Episode', 'y': 'Reward'},\n",
    "                title='Episode Rewards over Time').show()\n",
    "\n",
    "        action_freq = action_counts / action_counts.sum()\n",
    "        entropy = -np.sum(action_freq * np.log(action_freq + 1e-10))\n",
    "        print(f'Action distribution during training: {action_counts}')\n",
    "        print(f'Action entropy: {entropy:.3f} (max={np.log(env.action_space.n):.3f})')\n",
    "        print(f'Most used action: {np.argmax(action_counts)} ({action_counts.max() / action_counts.sum() * 100:.1f}%)')\n",
    "        print(f\"Training completed. Max score ever: {np.max(rewards)}\")\n",
    "\n",
    "        model.save(self._file_name_for_class(class_name))\n"
   ],
   "id": "fe4a830dbe66615c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.304964Z",
     "start_time": "2025-11-15T22:17:11.216335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_NAME = \"Berzerk\"\n",
    "\n",
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"rgb_array\", frameskip=4)\n",
    "agent = Sarsa(env.action_space.n)\n",
    "observation, info = env.reset()"
   ],
   "id": "7bae41ecb4ae1c5b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.324482Z",
     "start_time": "2025-11-15T22:17:11.314978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "epsilon_min = 0.1\n",
    "\n",
    "trainer = Trainer(epsilon_min=0.05, epsilon_decay_fraction=0.95, initial_epsilon=1.0)\n",
    "agent = trainer.train_if_needed(agent, env, class_name=CLASS_NAME, n_episodes=5000)\n",
    "\n",
    "env.close()"
   ],
   "id": "8140556e3832c90e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model file: sarsa-weights-berzerk.npz\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "8180a1812bf1eb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:11.381620Z",
     "start_time": "2025-11-15T22:17:11.376626Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa.load(Trainer._file_name_for_class(CLASS_NAME))",
   "id": "4128d9fb05e18a78",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:12.016473Z",
     "start_time": "2025-11-15T22:17:11.389627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale)\n",
    "\n",
    "test_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\", frameskip=4)\n",
    "agent.restrict_exploration()"
   ],
   "id": "80fd0e4d3717b2b3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-15T22:17:12.023447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "# agent.w = normalize_weights(agent.w)\n",
    "print(f'w={agent.w}')\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    actions_count = np.zeros(test_env.action_space.n, dtype=np.int32)\n",
    "    while not done:\n",
    "        feature_state = State(state)\n",
    "        if feature_state.is_empty:\n",
    "            action = 0\n",
    "        else:\n",
    "            action, _ = agent.epsilon_greedy(feature_state.as_vector())\n",
    "        actions_count[action] += 1\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "    test_env.render()\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward}\")\n",
    "    print(f'Action count during round: {actions_count}')\n",
    "    print('---------------------------')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "print(f\"\\nAverage Test Reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")"
   ],
   "id": "55994c398eb234cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[[-6.07160036e-05  4.70236177e-04 -8.69175128e-05 -1.07395416e-03\n",
      "   4.57342161e-04 -8.63641035e-04 -3.90825764e-04  1.04270228e-04\n",
      "  -1.71819446e-03]\n",
      " [-6.83609105e-04  3.23683926e-04 -6.80637488e-04 -2.53339368e-03\n",
      "   3.45227425e-04 -1.56091608e-03  1.47861851e-04  2.44536990e-04\n",
      "  -8.02226772e-04]\n",
      " [ 1.01006351e-01  5.31774163e-02  9.82944816e-02  1.11816106e-02\n",
      "   4.88153212e-02 -6.61352500e-02 -2.23695375e-02 -1.13753928e-02\n",
      "  -8.40030015e-02]\n",
      " [ 2.20728558e-04  4.89404541e-04  1.25715014e-04 -1.17655133e-03\n",
      "   4.68543090e-04 -1.18444453e-03 -1.86515725e-04  3.39203398e-04\n",
      "  -1.51710596e-03]\n",
      " [ 6.46755623e-04  6.89243607e-04  5.91686228e-04 -1.11239241e-03\n",
      "   6.56651682e-04 -1.52873073e-03  2.81978078e-04  6.83568724e-05\n",
      "  -4.02187376e-04]\n",
      " [ 4.45951335e-02  1.22795105e-02  4.24610935e-02  2.71789860e-02\n",
      "   1.04052080e-02 -2.18849834e-02 -2.25475058e-02 -1.01971226e-02\n",
      "  -8.30574259e-02]\n",
      " [ 2.92244600e-04  5.86364011e-04  2.44696363e-04 -1.54615846e-03\n",
      "   5.71419136e-04 -1.36415579e-03 -4.35368536e-04  1.18160046e-04\n",
      "  -1.67734898e-03]\n",
      " [ 4.96273744e-04  6.59717480e-04  4.69151826e-04 -8.06288153e-04\n",
      "   6.22151478e-04 -5.39539498e-04  1.99052258e-04  3.23793793e-04\n",
      "  -6.25325367e-04]\n",
      " [ 1.43638492e-04  3.62853432e-04  1.44898237e-04 -1.99148478e-03\n",
      "   3.62121471e-04 -2.17037043e-03 -6.51388313e-04 -2.84053705e-04\n",
      "  -2.82123685e-03]\n",
      " [ 6.26715366e-04  6.32634212e-04  6.14248973e-04 -7.40331248e-04\n",
      "   5.91735297e-04 -1.23568194e-03  1.77438807e-04  8.83125831e-05\n",
      "  -6.75625750e-04]\n",
      " [ 7.82835996e-04  6.61554222e-04  7.02187011e-04 -4.73117892e-04\n",
      "   6.11853146e-04 -5.35579398e-04  2.18896967e-04  4.99659218e-04\n",
      "   2.00827490e-04]\n",
      " [ 1.71605386e-02  4.62779626e-02  1.28158731e-02 -3.83302048e-02\n",
      "   4.51060645e-02  1.18060119e-01 -3.02232467e-02  1.99853837e-01\n",
      "   2.30486587e-01]\n",
      " [-5.10196878e-05  4.33259382e-04 -8.80371008e-05 -1.20407646e-03\n",
      "   4.24361089e-04 -8.06672731e-04  2.47983098e-05  3.86516243e-04\n",
      "  -9.84979561e-04]\n",
      " [ 4.52981852e-02  5.82574308e-03  4.37344275e-02  4.25196216e-02\n",
      "   3.91833624e-03 -8.96485336e-03  8.01556334e-02 -4.83213644e-03\n",
      "   9.21660289e-02]\n",
      " [ 5.78904874e-05  3.74501047e-04  3.06657384e-05 -1.36609923e-03\n",
      "   3.63820116e-04 -4.01295838e-04  1.45067213e-04  4.54290246e-04\n",
      "  -6.17024838e-04]\n",
      " [-7.25295453e-04  3.59498168e-04 -7.11878703e-04 -3.45005561e-03\n",
      "   3.99182754e-04 -2.11019372e-03 -5.14007988e-04  2.22797651e-04\n",
      "  -2.11133598e-03]\n",
      " [-8.25842508e-05  4.30701097e-04 -1.02103055e-04 -1.21336675e-03\n",
      "   4.20524942e-04 -1.14047655e-03 -2.91254983e-04  1.73314576e-04\n",
      "  -1.38337247e-03]\n",
      " [-2.94099300e-05  3.96044401e-04 -7.64747383e-05 -1.05086062e-03\n",
      "   3.81482387e-04 -1.00401160e-03 -6.44028405e-05  1.65648802e-04\n",
      "  -1.53471075e-03]]\n",
      "Episode 1: Total Reward = 250.0\n",
      "Action count during round: [ 19   0 180   0   0   0   0   0   0   0   0  71   0   5   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 2: Total Reward = 300.0\n",
      "Action count during round: [ 19   0 181   0   0   0   0   0   0   0   0  74   0   5   0   0   0   0]\n",
      "---------------------------\n",
      "Episode 3: Total Reward = 250.0\n",
      "Action count during round: [ 19   0 182   0   0   0   0   0   0   0   0  72   0   5   0   0   0   0]\n",
      "---------------------------\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
